{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from src.dataset.dataset_info import datasets\n",
    "from src.models import MyCNN, MyLSTM, MyGRU\n",
    "# from src.models.dense_nn import  MyDenseNN\n",
    "\n",
    "#specifying main configuration of the experiment\n",
    "multi_class = True\n",
    "with_network_features = False\n",
    "\n",
    "with_sort_timestamp = True\n",
    "sequence_length = 3\n",
    "with_cross_validation = True\n",
    "cross_validation_splits_num = 5\n",
    "\n",
    "# choosing the dataset\n",
    "dataset = datasets[0]\n",
    "name = dataset.name\n",
    "print(\"dataset: {}\".format(name))\n",
    "path = \"./datasets/preprocessed/{}.pkl\".format(name)\n",
    "# graph_path = \"./datasets/preprocessed/graph_{}.gexf\".format(name)\n",
    "\n",
    "# loading the dataframe\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input dimension of the training set\n",
    "input_dim = df.shape[1] - len(dataset.drop_columns) - len(dataset.weak_columns) - 1  # for the label_column\n",
    "\n",
    "if not with_network_features:\n",
    "    input_dim = input_dim - len(dataset.network_features)\n",
    "    \n",
    "# specifying the number of classes, since it is different from one dataset to another and also if binary or multi-class classification\n",
    "num_classes = 2\n",
    "if multi_class:\n",
    "    num_classes = len(df[\"Attack\"].unique())\n",
    "\n",
    "num_epochs = 30\n",
    "    \n",
    "dropped_columns = dataset.drop_columns\n",
    "dataset_name = dataset.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = []\n",
    "if with_network_features:\n",
    "    nf = dataset.network_features\n",
    "\n",
    "models = [\n",
    "    MyCNN(\n",
    "        input_dim=input_dim,\n",
    "        dataset_name=dataset_name,\n",
    "        num_classes=num_classes,\n",
    "        multi_class=multi_class,\n",
    "        network_features=nf,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=256,\n",
    "        early_stop_patience=10,\n",
    "    ),\n",
    "    # MyLSTM(\n",
    "    #     sequence_length=sequence_length,\n",
    "    #     input_dim=input_dim,\n",
    "    #     dataset_name=dataset_name,\n",
    "    #     num_classes=num_classes,\n",
    "    #     multi_class=multi_class,\n",
    "    #     network_features=nf,\n",
    "    #     use_generator=True,\n",
    "    #     epochs=num_epochs,\n",
    "    #     batch_size=256,,\n",
    "        # early_stop_patience=10,\n",
    "    # ),\n",
    "    # MyGRU(\n",
    "    #     sequence_length=sequence_length,\n",
    "    #     input_dim=input_dim,\n",
    "    #     dataset_name=dataset_name,\n",
    "    #     num_classes=num_classes,\n",
    "    #     multi_class=multi_class,\n",
    "    #     network_features=nf,\n",
    "    #     use_generator=True,\n",
    "    #     epochs=num_epochs,\n",
    "    #     batch_size=256,,\n",
    "        # early_stop_patience=10,\n",
    "    # )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"stratified k-fold cross validation - manual sequences\"\n",
    "results[\"multi_class\"] = multi_class\n",
    "results[\"with_sort_timestamp\"] = with_sort_timestamp\n",
    "results[\"sequence_length\"] = sequence_length\n",
    "results[\"with_cross_validation\"] = with_cross_validation\n",
    "results[\"cross_validation_splits_num\"] = cross_validation_splits_num\n",
    "results[\"with_network_features\"] = with_network_features\n",
    "results[\"network_features\"] = dataset.cn_measures\n",
    "\n",
    "results[\"dataset_name\"] = dataset_name\n",
    "results[\"input_dim\"] = input_dim\n",
    "results[\"dropped_columns\"] = dropped_columns\n",
    "results[\"num_dropped_columns\"] = len(dropped_columns)\n",
    "\n",
    "results[\"models\"] = {}\n",
    "results[\"average_acc\"] = {}\n",
    "results[\"average\"] = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if with_sort_timestamp:\n",
    "    df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n",
    "    df.sort_values(dataset.timestamp_col, inplace= True)\n",
    "\n",
    "labels_names = {0: \"benign\", 1: \"attack\"}\n",
    "if multi_class:\n",
    "    fac = pd.factorize(df[dataset.class_col])\n",
    "    labels_names = {index: value for index, value in enumerate(fac[1])}\n",
    "    print(f\"==>> labels_names: {labels_names}\")\n",
    "    df[dataset.label_col] = fac[0]  # type: ignore\n",
    "\n",
    "\n",
    "df.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "df.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "\n",
    "if not with_network_features:\n",
    "    df = df.drop(dataset.network_features, axis=1)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['Label'].to_numpy()\n",
    "df = df.drop([dataset.label_col], axis=1).to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tscv = TimeSeriesSplit(n_splits=cross_validation_splits_num)\n",
    "i = 0\n",
    "for train_index, test_index in tscv.split(df):\n",
    "    training_labels = labels[train_index]\n",
    "    print(f\"==>> train_index: {train_index}\")\n",
    "    print(f\"==>> training_labels: {training_labels.shape}\")\n",
    "    testing_labels = labels[test_index]\n",
    "    print(f\"==>> test_index: {test_index}\")\n",
    "    print(f\"==>> testing_labels: {testing_labels.shape}\")\n",
    "\n",
    "    i += 1\n",
    "    print(\"fold: {}\".format(i))\n",
    "    # print(\"train_index: {}\".format(train_index))\n",
    "    print(\"=====================================\")\n",
    "    print(\"=====================================\")\n",
    "    # print(\"fold: {}/{}\".format(i, len(list_of_dfs)))\n",
    "    print(\"fold: {}/{}\".format(i, cross_validation_splits_num))\n",
    "\n",
    "    for model in models:\n",
    "        print(\"training: {}\".format(model.model_name()))\n",
    "        print(\"sequential: {}\".format(model.sequential))\n",
    "\n",
    "        training = df[train_index]\n",
    "        testing = df[test_index]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        training = scaler.fit_transform(training)\n",
    "        testing = scaler.transform(testing)\n",
    "\n",
    "        model.build()\n",
    "        model.train(training,\n",
    "                    training_labels)  # type: ignore\n",
    "        predictions, prediction_time = model.predict(\n",
    "            testing)  # type: ignore\n",
    "        model_name, scores, class_report = model.evaluate(  # type: ignore\n",
    "            predictions,\n",
    "            testing_labels,\n",
    "            prediction_time\n",
    "        )\n",
    "        scores[\"fold\"] = i\n",
    "        if i == 1:\n",
    "            results[\"models\"][model_name] = {}\n",
    "            results[\"models\"][model_name][\"scores\"] = [scores]\n",
    "            results[\"models\"][model_name][\"class_report\"] = [class_report]\n",
    "        else:\n",
    "            results[\"models\"][model_name][\"scores\"].append(scores)\n",
    "            results[\"models\"][model_name][\"class_report\"].append(\n",
    "                class_report)\n",
    "        # results[str(i) + model_name] = scores\n",
    "        print(\"{}: {}\".format(model_name, scores))\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model.model_name()\n",
    "        average_acc = 0\n",
    "        average_recall = 0\n",
    "        average_precision = 0\n",
    "        average_f1s = 0\n",
    "        average_FPR = 0\n",
    "        average_FNR = 0\n",
    "        for result in results[\"models\"][model_name][\"scores\"]:  # type: ignore\n",
    "            average_acc += result[\"accuracy\"]\n",
    "            average_recall += result[\"recall\"]\n",
    "            average_precision += result[\"precision\"]\n",
    "            average_f1s += result[\"f1s\"]\n",
    "            average_FPR += result[\"FPR\"]\n",
    "            average_FNR += result[\"FNR\"]\n",
    "        average_acc = average_acc / i\n",
    "        average_recall = average_recall / i\n",
    "        average_precision = average_precision / i\n",
    "        average_f1s = average_f1s / i\n",
    "        average_FPR = average_FPR / i\n",
    "        average_FNR = average_FNR / i\n",
    "        if i == 1:\n",
    "            results[\"models\"][model_name][\"average\"] = [\n",
    "                {\n",
    "                    \"average_acc\": average_acc,\n",
    "                    \"average_recall\": average_recall,\n",
    "                    \"average_precision\": average_precision,\n",
    "                    \"average_f1s\": average_f1s,\n",
    "                    \"average_FPR\": average_FPR,\n",
    "                    \"average_FNR\": average_FNR,\n",
    "                    \"fold\": i\n",
    "                }\n",
    "            ]\n",
    "            results[\"average_acc\"][model_name] = average_acc\n",
    "            results[\"average\"][model_name] = {\n",
    "                \"average_acc\": average_acc,\n",
    "                \"average_recall\": average_recall,\n",
    "                \"average_precision\": average_precision,\n",
    "                \"average_f1s\": average_f1s,\n",
    "                \"average_FPR\": average_FPR,\n",
    "                \"average_FNR\": average_FNR\n",
    "            }\n",
    "        else:\n",
    "            results[\"models\"][model_name][\"average\"].append(\n",
    "                {\n",
    "                    \"average_acc\": average_acc,\n",
    "                    \"average_recall\": average_recall,\n",
    "                    \"average_precision\": average_precision,\n",
    "                    \"average_f1s\": average_f1s,\n",
    "                    \"average_FPR\": average_FPR,\n",
    "                    \"average_FNR\": average_FNR,\n",
    "                    \"fold\": i\n",
    "                })\n",
    "            results[\"average_acc\"][model_name] = average_acc\n",
    "            results[\"average\"][model_name] = {\n",
    "                \"average_acc\": average_acc,\n",
    "                \"average_recall\": average_recall,\n",
    "                \"average_precision\": average_precision,\n",
    "                \"average_f1s\": average_f1s,\n",
    "                \"average_FPR\": average_FPR,\n",
    "                \"average_FNR\": average_FNR\n",
    "            }\n",
    "        print(\"{} average accuracy: {}\".format(model_name, average_acc))\n",
    "\n",
    "results[\"endtime\"] = time.strftime(\"%Y:%m:%d-%H:%M:%S\")\n",
    "\n",
    "print(f\"==>> results: {results}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "if not os.path.isdir('./results/{}'.format(dataset_name)):\n",
    "    os.mkdir('./results/{}'.format(dataset_name))\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# saving the results to a file for future refernece\n",
    "filename = ('./results/{}/{}.json'.format(dataset_name,\n",
    "            time.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
