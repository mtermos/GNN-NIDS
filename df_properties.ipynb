{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "\n",
    "from src.dataset.dataset_info import datasets\n",
    "from src.numpy_encoder import NumpyEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"cic_ton_iot_5_percent\"\n",
    "# name = \"cic_ton_iot\"\n",
    "name = \"cic_ids_2017_5_percent\"\n",
    "# name = \"cic_ids_2017\"\n",
    "# name = \"cic_bot_iot\"\n",
    "# name = \"cic_ton_iot_modified\"\n",
    "# name = \"nf_ton_iotv2_modified\"\n",
    "# name = \"ccd_inid_modified\"\n",
    "# name = \"nf_uq_nids_modified\"\n",
    "# name = \"edge_iiot\"\n",
    "# name = \"nf_cse_cic_ids2018\"\n",
    "# name = \"nf_bot_iotv2\"\n",
    "# name = \"nf_uq_nids\"\n",
    "# name = \"x_iiot\"\n",
    "\n",
    "dataset = datasets[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(dataset.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "data_s = \"dataset\"\n",
    "multi_g = \"multi_di_graph\"\n",
    "multi_g_with_ports = \"multi_di_graph_with_ports\"\n",
    "di_g = \"di_graph\"\n",
    "di_g_with_ports = \"di_graph_with_ports\"\n",
    "\n",
    "results[\"name\"] = name\n",
    "results[data_s] = {}\n",
    "results[multi_g] = {}\n",
    "results[multi_g_with_ports] = {}\n",
    "results[di_g] = {}\n",
    "results[di_g_with_ports] = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Gini coefficient\n",
    "def gini_coefficient(values):\n",
    "    sorted_values = np.sort(values)\n",
    "    n = len(sorted_values)\n",
    "    cumulative_values = np.cumsum(sorted_values)\n",
    "    gini = (n + 1 - 2 * np.sum(cumulative_values) / np.sum(sorted_values)) / n\n",
    "    return gini\n",
    "\n",
    "# Multi-class Gini Coefficient\n",
    "class_counts = df[dataset.class_num_col].value_counts()\n",
    "class_proportions = class_counts.values / class_counts.values.sum()\n",
    "multi_class_gini = gini_coefficient(class_proportions)\n",
    "\n",
    "print(\"Class Counts:\")\n",
    "print(class_counts)\n",
    "print(\"Class Proportions:\", class_proportions)\n",
    "print(\"Multi-class Gini Coefficient:\", multi_class_gini)\n",
    "\n",
    "# Binary Classification Gini Coefficient\n",
    "label_counts = df[dataset.label_col].value_counts()\n",
    "label_proportions = label_counts.values / label_counts.values.sum()\n",
    "binary_gini = gini_coefficient(label_proportions)\n",
    "\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print(\"Label Proportions:\", label_proportions)\n",
    "print(\"Binary Classification Gini Coefficient:\", binary_gini)\n",
    "\n",
    "\n",
    "results[data_s][\"Multi-class Gini Coefficient\"] = multi_class_gini\n",
    "results[data_s][\"Binary Classification Gini Coefficient\"] = binary_gini\n",
    "\n",
    "total_count = len(df)\n",
    "results[data_s][\"length\"] = total_count\n",
    "num_benign = len(df[df[dataset.label_col] == 0])\n",
    "num_attack = len(df[df[dataset.label_col] == 1])\n",
    "\n",
    "results[data_s][\"num_benign\"] = num_benign\n",
    "results[data_s][\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "results[data_s][\"num_attack\"] = num_attack\n",
    "results[data_s][\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "results[data_s][\"attacks\"] = list(df[dataset.class_col].unique())\n",
    "\n",
    "# Interpretation:\n",
    "# - A Gini coefficient closer to 0 indicates balanced distribution.\n",
    "# - A Gini coefficient closer to 1 indicates imbalanced distribution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_pairs(df_cs, source_ip, destination_ip, class_column, results_dict, graph_name, folder_path):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    same_class_pairs = {}\n",
    "    mixed_class_pairs = []\n",
    "\n",
    "    # Group by source and destination IP addresses\n",
    "    for (source, destination), group in df_cs.groupby([source_ip, destination_ip]):\n",
    "        unique_classes = group[class_column].unique()\n",
    "        if len(unique_classes) == 1:\n",
    "            # All records have the same class\n",
    "            class_label = str(unique_classes[0])\n",
    "            if class_label not in same_class_pairs:\n",
    "                same_class_pairs[class_label] = []\n",
    "            same_class_pairs[class_label].append({\n",
    "                'node_pair': (source, destination),\n",
    "                'num_instances': len(group)\n",
    "            })\n",
    "        else:\n",
    "            # Mixed class scenario\n",
    "            class_counts = group[class_column].value_counts().to_dict()\n",
    "            total_instances = len(group)\n",
    "            class_percentages = {str(cls): count / total_instances for cls, count in class_counts.items()}\n",
    "            mixed_class_pairs.append({\n",
    "                'node_pair': (source, destination),\n",
    "                'class_counts': class_counts,\n",
    "                'class_percentages': class_percentages\n",
    "            })\n",
    "\n",
    "\n",
    "    # Output results\n",
    "    # print(\"Node pairs with the same class:\")\n",
    "    # for class_label, pairs in same_class_pairs.items():\n",
    "    #     print(f\"Class {class_label}: {pairs}\")\n",
    "\n",
    "    # print(\"\\nNode pairs with mixed classes:\")\n",
    "    # for mixed_pair in mixed_class_pairs:\n",
    "    #     print(mixed_pair)\n",
    "    with open(os.path.join(folder_path, f\"{graph_name}_same_class_pairs.json\"), \"w\") as f:\n",
    "        f.writelines(json.dumps(same_class_pairs, cls=NumpyEncoder))\n",
    "        \n",
    "    with open(os.path.join(folder_path, f\"{graph_name}_mixed_class_pairs.json\"), \"w\") as f:\n",
    "        f.writelines(json.dumps(mixed_class_pairs, cls=NumpyEncoder))\n",
    "\n",
    "    # Total counts\n",
    "    total_same_class_pairs = sum(len(pairs) for pairs in same_class_pairs.values())\n",
    "    total_mixed_class_pairs = len(mixed_class_pairs)\n",
    "\n",
    "    print(\"\\nTotal number of same class pairs:\", total_same_class_pairs)\n",
    "    print(\"Total number of mixed class pairs:\", total_mixed_class_pairs)\n",
    "    \n",
    "    results_dict[graph_name][\"total_same_class_pairs\"] = total_same_class_pairs\n",
    "    results_dict[graph_name][\"total_mixed_class_pairs\"] = total_mixed_class_pairs\n",
    "\n",
    "    # Interpretation:\n",
    "    # - `same_class_pairs` contains node pairs with consistent classes across all records, including the number of instances.\n",
    "    # - `mixed_class_pairs` contains node pairs with mixed classes, the counts and percentages for each class.\n",
    "    # - Total counts provide an overview of the dataset's class consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_classes = os.path.join(\"datasets\", name, \"class_pairs\")\n",
    "class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_col, results, multi_g, folder_path_classes)\n",
    "class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.label_col, results, di_g, folder_path_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df, dataset.src_ip_col, dataset.dst_ip_col, edge_attr=[dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
    "simple_digraph = nx.DiGraph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
    "df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n",
    "\n",
    "df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
    "df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]\n",
    "\n",
    "G_with_ports = nx.from_pandas_edgelist(df, dataset.src_ip_col, dataset.dst_ip_col, edge_attr=[dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
    "simple_digraph_with_ports = nx.DiGraph(G_with_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_col, results, multi_g_with_ports, folder_path_classes)\n",
    "class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.label_col, results, di_g_with_ports, folder_path_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[multi_g][\"is_strongly_connected\"] = nx.is_strongly_connected(G)\n",
    "results[multi_g_with_ports][\"is_strongly_connected\"] = nx.is_strongly_connected(G_with_ports)\n",
    "results[di_g][\"is_strongly_connected\"] = nx.is_strongly_connected(simple_digraph)\n",
    "results[di_g_with_ports][\"is_strongly_connected\"] = nx.is_strongly_connected(simple_digraph_with_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_centrality_skewness(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Compute degree centrality\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "\n",
    "    # Extract the values of degree centrality\n",
    "    degree_values = list(degree_centrality.values())\n",
    "\n",
    "    # Calculate skewness and kurtosis\n",
    "    degree_skewness = skew(degree_values)\n",
    "    degree_kurtosis = kurtosis(degree_values, fisher=True)  # Fisher=True returns excess kurtosis\n",
    "\n",
    "    print(graph_name, \" Skewness of Degree Centrality:\", degree_skewness)\n",
    "    print(graph_name, \" Kurtosis of Degree Centrality:\", degree_kurtosis)\n",
    "\n",
    "    results_dict[graph_name][\"degree_skewness\"] = degree_skewness\n",
    "    results_dict[graph_name][\"degree_kurtosis\"] = degree_kurtosis\n",
    "    # Interpretation:\n",
    "    # - A high positive skewness indicates a long tail on the right (few nodes with very high centrality).\n",
    "    # - A high kurtosis indicates heavy tails or a highly peaked distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality_skewness(G, results, multi_g)\n",
    "degree_centrality_skewness(simple_digraph, results, di_g)\n",
    "degree_centrality_skewness(G_with_ports, results, multi_g_with_ports)\n",
    "degree_centrality_skewness(simple_digraph_with_ports, results, di_g_with_ports)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, \"df_properties_new.json\"), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attackers / Victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attackers_victims(graph, results_dict, graph_name, label_col):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Step 1: Identify unique nodes involved in attack and normal traffic\n",
    "    attackers = set()\n",
    "    victims = set()\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if data[label_col] == 1:\n",
    "            attackers.add(u)\n",
    "            victims.add(v)\n",
    "\n",
    "    # Step 2: Count unique attackers and victims\n",
    "    num_attackers = len(attackers)\n",
    "    num_victims = len(victims)\n",
    "\n",
    "    # Step 3: Calculate proportions\n",
    "    total_nodes = graph.number_of_nodes()\n",
    "    attacker_proportion = num_attackers / total_nodes if total_nodes > 0 else 0\n",
    "    victim_proportion = num_victims / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "    # print(graph_name, \" Unique Attackers:\", attackers)\n",
    "    # print(graph_name, \" Unique Victims:\", victims)\n",
    "    print(graph_name, \" Number of Attackers:\", num_attackers)\n",
    "    print(graph_name, \" Number of Victims:\", num_victims)\n",
    "    print(graph_name, \" Proportion of Attackers:\", attacker_proportion)\n",
    "    print(graph_name, \" Proportion of Victims:\", victim_proportion)\n",
    "\n",
    "    results_dict[graph_name][\"total_nodes\"] = total_nodes\n",
    "    results_dict[graph_name][\"Number of Attackers\"] = num_attackers\n",
    "    results_dict[graph_name][\"Number of Victims\"] = num_victims\n",
    "    results_dict[graph_name][\"Proportion of Attackers\"] = attacker_proportion\n",
    "    results_dict[graph_name][\"Proportion of Victims\"] = victim_proportion\n",
    "    results_dict[graph_name][\"intersection between attacks and victims\"] = len(attackers.intersection(victims))\n",
    "\n",
    "\n",
    "    # Interpretation:\n",
    "    # - Attackers: Source nodes of edges labeled as \"Attack\".\n",
    "    # - Victims: Target nodes of edges labeled as \"Attack\".\n",
    "    # - These metrics provide insight into the roles of nodes in attack scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackers_victims(G, results, multi_g, dataset.label_col)\n",
    "# attackers_victims(simple_digraph, results, di_g, dataset.label_col)\n",
    "attackers_victims(G_with_ports, results, multi_g_with_ports, dataset.label_col)\n",
    "# attackers_victims(simple_digraph_with_ports, results, di_g_with_ports, dataset.label_col)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, \"df_properties_new.json\"), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Metrics Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_clustering_coefficients(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Clustering Coefficient Distribution Metric\n",
    "    clustering_coefficients = nx.clustering(nx.Graph(graph))  # Convert MultiDiGraph to Graph for clustering\n",
    "    clustering_values = list(clustering_coefficients.values())\n",
    "    mean_clustering = np.mean(clustering_values)\n",
    "    std_clustering = np.std(clustering_values)\n",
    "\n",
    "    # print(\"Clustering Coefficients:\", clustering_coefficients)\n",
    "    print(graph_name, \" Mean Clustering Coefficient:\", mean_clustering)\n",
    "    print(graph_name, \" Standard Deviation of Clustering Coefficients:\", std_clustering)\n",
    "\n",
    "    results_dict[graph_name][\"Mean Clustering Coefficient\"] = mean_clustering\n",
    "    results_dict[graph_name][\"Standard Deviation of Clustering Coefficients\"] = std_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_clustering_coefficients(G, results, multi_g)\n",
    "cal_clustering_coefficients(simple_digraph, results, di_g)\n",
    "cal_clustering_coefficients(G_with_ports, results, multi_g_with_ports)\n",
    "cal_clustering_coefficients(simple_digraph_with_ports, results, di_g_with_ports)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, \"df_properties_new.json\"), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_degree_assortativity(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Graph Assortativity Metric\n",
    "    try:\n",
    "        degree_assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "        results_dict[graph_name][\"Graph Degree Assortativity Coefficient\"] = degree_assortativity\n",
    "        print(graph_name, \" Degree Assortativity Coefficient:\", degree_assortativity)\n",
    "    except nx.NetworkXError as e:\n",
    "        results_dict[graph_name][\"Graph Degree Assortativity Coefficient\"] = \"not applicable\"\n",
    "        print(graph_name, \" Error calculating assortativity:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_degree_assortativity(G, results, multi_g)\n",
    "cal_degree_assortativity(simple_digraph, results, di_g)\n",
    "cal_degree_assortativity(G_with_ports, results, multi_g_with_ports)\n",
    "cal_degree_assortativity(simple_digraph_with_ports, results, di_g_with_ports)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, \"df_properties_new.json\"), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_diameter(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Graph Diameter Metric\n",
    "    try:\n",
    "        if nx.is_strongly_connected(graph):\n",
    "            diameter = nx.diameter(graph)\n",
    "            results_dict[graph_name][\"diameter\"] = diameter\n",
    "            print(graph_name, \" Graph Diameter multidigraph:\", diameter)\n",
    "        else:\n",
    "            results_dict[graph_name][\"diameter\"] = \"not applicable\"\n",
    "            print(graph_name, \" Graph is not strongly connected, diameter is undefined.\")\n",
    "    \n",
    "    except nx.NetworkXError as e:\n",
    "        print(\"Error calculating diameter:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_diameter(G, results, multi_g)\n",
    "cal_diameter(simple_digraph, results, di_g)\n",
    "cal_diameter(G_with_ports, results, multi_g_with_ports)\n",
    "cal_diameter(simple_digraph_with_ports, results, di_g_with_ports)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, \"df_properties_new.json\"), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_length_distribution(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Path Length Distribution Metric\n",
    "    try:\n",
    "        path_lengths = dict(nx.all_pairs_shortest_path_length(graph))\n",
    "        all_lengths = [length for source in path_lengths.values() for length in source.values()]\n",
    "        mean_path_length = np.mean(all_lengths)\n",
    "        std_path_length = np.std(all_lengths)\n",
    "\n",
    "        print(graph_name, \" Mean Path Length MultiDiGraph:\", mean_path_length)\n",
    "        print(graph_name, \" Standard Deviation of Path Lengths MultiDiGraph:\", std_path_length)\n",
    "        results_dict[graph_name][\"Mean Path Length\"] = mean_path_length\n",
    "        results_dict[graph_name][\"Standard Deviation of Path Lengths\"] = std_path_length\n",
    "        \n",
    "    except nx.NetworkXError as e:\n",
    "        results_dict[graph_name][\"Mean Path Length\"] = \"not applicable\"\n",
    "        results_dict[graph_name][\"Standard Deviation of Path Lengths\"] = \"not applicable\"\n",
    "        print(graph_name, \" Error calculating path length distribution:\", e)\n",
    "\n",
    "    # Interpretation:\n",
    "    # - Diameter: Longest shortest path in the graph (undefined for disconnected graphs).\n",
    "    # - Assortativity: Correlation of node degrees (positive, negative, or neutral).\n",
    "    # - Clustering Coefficients: Measure of local connectivity (distribution provides network structure insights).\n",
    "    # - Path Lengths: Reachability analysis using shortest paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_length_distribution(G, results, multi_g)\n",
    "path_length_distribution(simple_digraph, results, di_g)\n",
    "# path_length_distribution(G_with_ports, results, multi_g_with_ports)\n",
    "# path_length_distribution(simple_digraph_with_ports, results, di_g_with_ports)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"datasets\", name, \"df_properties_new.json\"), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
