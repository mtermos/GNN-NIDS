{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "\n",
    "from src.dataset.dataset_info import datasets\n",
    "from src.numpy_encoder import NumpyEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"cic_ton_iot_5_percent\"\n",
    "# name = \"cic_ton_iot\"\n",
    "# name = \"cic_ids_2017_5_percent\"\n",
    "# name = \"cic_ids_2017\"\n",
    "# name = \"cic_bot_iot\"\n",
    "# name = \"cic_ton_iot_modified\"\n",
    "# name = \"nf_ton_iotv2_modified\"\n",
    "# name = \"ccd_inid_modified\"\n",
    "# name = \"nf_uq_nids_modified\"\n",
    "# name = \"edge_iiot\"\n",
    "name = \"nf_cse_cic_ids2018\"\n",
    "# name = \"nf_bot_iotv2\"\n",
    "# name = \"nf_uq_nids\"\n",
    "# name = \"x_iiot\"\n",
    "\n",
    "dataset = datasets[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(dataset.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphsType:\n",
    "    def __init__(self, name, nx_type, graph = None, with_ports = False):\n",
    "        self.name = name\n",
    "        self.nx_type = nx_type\n",
    "        self.graph = graph\n",
    "        self.with_ports = with_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "data_s = \"dataset\"\n",
    "\n",
    "new_file_name = \"df_properties_new.json\"\n",
    "graphs_types = [\n",
    "    GraphsType(\"multi_di_graph\", nx.MultiDiGraph),\n",
    "    # GraphsType(\"multi_di_graph_with_ports\", nx.MultiDiGraph, with_ports = True),\n",
    "    # GraphsType(\"di_graph\", nx.DiGraph),\n",
    "    # GraphsType(\"di_graph_with_ports\", nx.DiGraph, with_ports = True),\n",
    "]\n",
    "\n",
    "results[\"name\"] = name\n",
    "results[data_s] = {}\n",
    "\n",
    "for g in graphs_types:\n",
    "    results[g.name] = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      "Attack\n",
      "Benign                      6682887\n",
      "DDoS attacks-LOIC-HTTP       297364\n",
      "DoS attacks-Hulk             108129\n",
      "SSH-Bruteforce                71148\n",
      "Infilteration                 59505\n",
      "DoS attacks-GoldenEye         32582\n",
      "DoS attacks-Slowloris         17109\n",
      "Bot                           15498\n",
      "DoS attacks-SlowHTTPTest      14116\n",
      "FTP-BruteForce                14116\n",
      "DDOS attack-LOIC-UDP           1667\n",
      "DDOS attack-HOIC                230\n",
      "Brute Force -Web                173\n",
      "Brute Force -XSS                101\n",
      "SQL Injection                    36\n",
      "Name: count, dtype: int64\n",
      "Class Proportions: [9.13629080e-01 4.06531485e-02 1.47825032e-02 9.72676656e-03\n",
      " 8.13503182e-03 4.45434177e-03 2.33900108e-03 2.11875848e-03\n",
      " 1.92982286e-03 1.92982286e-03 2.27898463e-04 3.14436992e-05\n",
      " 2.36511302e-05 1.38078853e-05 4.92162248e-06]\n",
      "Multi-class Gini Coefficient: 0.9041280427532231\n",
      "Label Counts:\n",
      "Label\n",
      "0    6682887\n",
      "1     631774\n",
      "Name: count, dtype: int64\n",
      "Label Proportions: [0.91362908 0.08637092]\n",
      "Binary Classification Gini Coefficient: 0.4136290800079456\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the Gini coefficient\n",
    "def gini_coefficient(values):\n",
    "    sorted_values = np.sort(values)\n",
    "    n = len(sorted_values)\n",
    "    cumulative_values = np.cumsum(sorted_values)\n",
    "    gini = (n + 1 - 2 * np.sum(cumulative_values) / np.sum(sorted_values)) / n\n",
    "    return gini\n",
    "\n",
    "# Multi-class Gini Coefficient\n",
    "class_counts = df[dataset.class_col].value_counts()\n",
    "class_proportions = class_counts.values / class_counts.values.sum()\n",
    "multi_class_gini = gini_coefficient(class_proportions)\n",
    "\n",
    "print(\"Class Counts:\")\n",
    "print(class_counts)\n",
    "print(\"Class Proportions:\", class_proportions)\n",
    "print(\"Multi-class Gini Coefficient:\", multi_class_gini)\n",
    "\n",
    "results[data_s][\"Class Counts / Proportions\"] = {\n",
    "    class_name: {\n",
    "        \"count\": int(count),\n",
    "        \"proportion\": float(proportion)\n",
    "    }\n",
    "    for class_name, count, proportion in zip(class_counts.index, class_counts.values, class_proportions)\n",
    "}\n",
    "# results[data_s][\"Class Proportions\"] = class_proportions\n",
    "\n",
    "# Binary Classification Gini Coefficient\n",
    "label_counts = df[dataset.label_col].value_counts()\n",
    "label_proportions = label_counts.values / label_counts.values.sum()\n",
    "binary_gini = gini_coefficient(label_proportions)\n",
    "\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print(\"Label Proportions:\", label_proportions)\n",
    "print(\"Binary Classification Gini Coefficient:\", binary_gini)\n",
    "\n",
    "\n",
    "results[data_s][\"Multi-class Gini Coefficient\"] = multi_class_gini\n",
    "results[data_s][\"Binary Classification Gini Coefficient\"] = binary_gini\n",
    "\n",
    "total_count = len(df)\n",
    "results[data_s][\"length\"] = total_count\n",
    "num_benign = len(df[df[dataset.label_col] == 0])\n",
    "num_attack = len(df[df[dataset.label_col] == 1])\n",
    "\n",
    "results[data_s][\"num_benign\"] = num_benign\n",
    "results[data_s][\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "results[data_s][\"num_attack\"] = num_attack\n",
    "results[data_s][\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "results[data_s][\"attacks\"] = list(df[dataset.class_col].unique())\n",
    "\n",
    "# Interpretation:\n",
    "# - A Gini coefficient closer to 0 indicates balanced distribution.\n",
    "# - A Gini coefficient closer to 1 indicates imbalanced distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'nf_cse_cic_ids2018',\n",
       " 'dataset': {'Class Counts / Proportions': {'Benign': {'count': 6682887,\n",
       "    'proportion': 0.9136290800079457},\n",
       "   'DDoS attacks-LOIC-HTTP': {'count': 297364,\n",
       "    'proportion': 0.04065314851911797},\n",
       "   'DoS attacks-Hulk': {'count': 108129, 'proportion': 0.014782503249296175},\n",
       "   'SSH-Bruteforce': {'count': 71148, 'proportion': 0.00972676655828616},\n",
       "   'Infilteration': {'count': 59505, 'proportion': 0.008135031821707116},\n",
       "   'DoS attacks-GoldenEye': {'count': 32582,\n",
       "    'proportion': 0.00445434176648788},\n",
       "   'DoS attacks-Slowloris': {'count': 17109,\n",
       "    'proportion': 0.002339001082893657},\n",
       "   'Bot': {'count': 15498, 'proportion': 0.0021187584769820503},\n",
       "   'DoS attacks-SlowHTTPTest': {'count': 14116,\n",
       "    'proportion': 0.0019298228585029435},\n",
       "   'FTP-BruteForce': {'count': 14116, 'proportion': 0.0019298228585029435},\n",
       "   'DDOS attack-LOIC-UDP': {'count': 1667,\n",
       "    'proportion': 0.00022789846310034052},\n",
       "   'DDOS attack-HOIC': {'count': 230, 'proportion': 3.144369916801339e-05},\n",
       "   'Brute Force -Web': {'count': 173, 'proportion': 2.3651130243766595e-05},\n",
       "   'Brute Force -XSS': {'count': 101, 'proportion': 1.3807885286823272e-05},\n",
       "   'SQL Injection': {'count': 36, 'proportion': 4.921622478471661e-06}},\n",
       "  'Multi-class Gini Coefficient': np.float64(0.9041280427532231),\n",
       "  'Binary Classification Gini Coefficient': np.float64(0.4136290800079456),\n",
       "  'length': 7314661,\n",
       "  'num_benign': 6682887,\n",
       "  'percentage_of_benign_records': 91.36290800079456,\n",
       "  'num_attack': 631774,\n",
       "  'percentage_of_attack_records': 8.637091999205431,\n",
       "  'attacks': ['Benign',\n",
       "   'Infilteration',\n",
       "   'Bot',\n",
       "   'Brute Force -Web',\n",
       "   'Brute Force -XSS',\n",
       "   'DDOS attack-HOIC',\n",
       "   'SQL Injection',\n",
       "   'DoS attacks-Hulk',\n",
       "   'DoS attacks-SlowHTTPTest',\n",
       "   'DDoS attacks-LOIC-HTTP',\n",
       "   'DDOS attack-LOIC-UDP',\n",
       "   'DoS attacks-GoldenEye',\n",
       "   'DoS attacks-Slowloris',\n",
       "   'FTP-BruteForce',\n",
       "   'SSH-Bruteforce']},\n",
       " 'multi_di_graph': {}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_pairs(df_cs, source_ip, destination_ip, class_column, results_dict, graph_name, folder_path):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    same_class_pairs = {}\n",
    "    mixed_class_pairs = []\n",
    "\n",
    "    # Group by source and destination IP addresses\n",
    "    for (source, destination), group in df_cs.groupby([source_ip, destination_ip]):\n",
    "        unique_classes = group[class_column].unique()\n",
    "        if len(unique_classes) == 1:\n",
    "            # All records have the same class\n",
    "            class_label = str(unique_classes[0])\n",
    "            if class_label not in same_class_pairs:\n",
    "                same_class_pairs[class_label] = []\n",
    "            same_class_pairs[class_label].append({\n",
    "                'node_pair': (source, destination),\n",
    "                'num_instances': len(group)\n",
    "            })\n",
    "        else:\n",
    "            # Mixed class scenario\n",
    "            class_counts = group[class_column].value_counts().to_dict()\n",
    "            total_instances = len(group)\n",
    "            class_percentages = {str(cls): count / total_instances for cls, count in class_counts.items()}\n",
    "            mixed_class_pairs.append({\n",
    "                'node_pair': (source, destination),\n",
    "                'class_counts': class_counts,\n",
    "                'class_percentages': class_percentages\n",
    "            })\n",
    "\n",
    "\n",
    "    # Output results\n",
    "    # print(\"Node pairs with the same class:\")\n",
    "    # for class_label, pairs in same_class_pairs.items():\n",
    "    #     print(f\"Class {class_label}: {pairs}\")\n",
    "\n",
    "    # print(\"\\nNode pairs with mixed classes:\")\n",
    "    # for mixed_pair in mixed_class_pairs:\n",
    "    #     print(mixed_pair)\n",
    "    with open(os.path.join(folder_path, f\"{graph_name}_same_class_pairs.json\"), \"w\") as f:\n",
    "        f.writelines(json.dumps(same_class_pairs, cls=NumpyEncoder))\n",
    "        \n",
    "    with open(os.path.join(folder_path, f\"{graph_name}_mixed_class_pairs.json\"), \"w\") as f:\n",
    "        f.writelines(json.dumps(mixed_class_pairs, cls=NumpyEncoder))\n",
    "\n",
    "    # Total counts\n",
    "    total_same_class_pairs = sum(len(pairs) for pairs in same_class_pairs.values())\n",
    "    total_mixed_class_pairs = len(mixed_class_pairs)\n",
    "\n",
    "    print(\"\\nTotal number of same class pairs:\", total_same_class_pairs)\n",
    "    print(\"Total number of mixed class pairs:\", total_mixed_class_pairs)\n",
    "    \n",
    "    results_dict[graph_name][\"total_same_class_pairs\"] = total_same_class_pairs\n",
    "    results_dict[graph_name][\"total_mixed_class_pairs\"] = total_mixed_class_pairs\n",
    "\n",
    "    # Interpretation:\n",
    "    # - `same_class_pairs` contains node pairs with consistent classes across all records, including the number of instances.\n",
    "    # - `mixed_class_pairs` contains node pairs with mixed classes, the counts and percentages for each class.\n",
    "    # - Total counts provide an overview of the dataset's class consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "\n",
      "Total number of same class pairs: 464278\n",
      "Total number of mixed class pairs: 2147\n"
     ]
    }
   ],
   "source": [
    "folder_path_classes = os.path.join(\"datasets\", name, \"class_pairs\")\n",
    "\n",
    "for g in graphs_types:\n",
    "    if not g.with_ports:\n",
    "        class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_col, results, g.name, folder_path_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    if not g.with_ports:\n",
    "        g.graph = nx.from_pandas_edgelist(df, dataset.src_ip_col, dataset.dst_ip_col, edge_attr=[dataset.label_col, dataset.class_num_col], create_using=g.nx_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(g.with_ports for g in graphs_types):\n",
    "    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
    "    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n",
    "\n",
    "    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
    "    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    if g.with_ports:\n",
    "        g.graph = nx.from_pandas_edgelist(df, dataset.src_ip_col, dataset.dst_ip_col, edge_attr=[dataset.label_col, dataset.class_num_col], create_using=g.nx_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    if g.with_ports:\n",
    "        class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_col, results, g.name, folder_path_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    results[g.name][\"is_strongly_connected\"] = nx.is_strongly_connected(g.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_centrality_skewness(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Compute degree centrality\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "\n",
    "    # Extract the values of degree centrality\n",
    "    degree_values = list(degree_centrality.values())\n",
    "\n",
    "    # Calculate skewness and kurtosis\n",
    "    degree_skewness = skew(degree_values)\n",
    "    degree_kurtosis = kurtosis(degree_values, fisher=True)  # Fisher=True returns excess kurtosis\n",
    "\n",
    "    print(graph_name, \" Skewness of Degree Centrality:\", degree_skewness)\n",
    "    print(graph_name, \" Kurtosis of Degree Centrality:\", degree_kurtosis)\n",
    "\n",
    "    results_dict[graph_name][\"degree_skewness\"] = degree_skewness\n",
    "    results_dict[graph_name][\"degree_kurtosis\"] = degree_kurtosis\n",
    "    # Interpretation:\n",
    "    # - A high positive skewness indicates a long tail on the right (few nodes with very high centrality).\n",
    "    # - A high kurtosis indicates heavy tails or a highly peaked distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Skewness of Degree Centrality: 225.01525590885825\n",
      "multi_di_graph  Kurtosis of Degree Centrality: 57051.08391489221\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    degree_centrality_skewness(g.graph, results, g.name)\n",
    "    \n",
    "with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attackers / Victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attackers_victims(graph, results_dict, graph_name, label_col):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Step 1: Identify unique nodes involved in attack and normal traffic\n",
    "    attackers = set()\n",
    "    victims = set()\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if data[label_col] == 1:\n",
    "            attackers.add(u)\n",
    "            victims.add(v)\n",
    "\n",
    "    # Step 2: Count unique attackers and victims\n",
    "    num_attackers = len(attackers)\n",
    "    num_victims = len(victims)\n",
    "\n",
    "    # Step 3: Calculate proportions\n",
    "    total_nodes = graph.number_of_nodes()\n",
    "    attacker_proportion = num_attackers / total_nodes if total_nodes > 0 else 0\n",
    "    victim_proportion = num_victims / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "    # print(graph_name, \" Unique Attackers:\", attackers)\n",
    "    # print(graph_name, \" Unique Victims:\", victims)\n",
    "    print(graph_name, \" Number of Attackers:\", num_attackers)\n",
    "    print(graph_name, \" Number of Victims:\", num_victims)\n",
    "    print(graph_name, \" Proportion of Attackers:\", attacker_proportion)\n",
    "    print(graph_name, \" Proportion of Victims:\", victim_proportion)\n",
    "\n",
    "    results_dict[graph_name][\"total_nodes\"] = total_nodes\n",
    "    results_dict[graph_name][\"Number of Attackers\"] = num_attackers\n",
    "    results_dict[graph_name][\"Number of Victims\"] = num_victims\n",
    "    results_dict[graph_name][\"Proportion of Attackers\"] = attacker_proportion\n",
    "    results_dict[graph_name][\"Proportion of Victims\"] = victim_proportion\n",
    "    results_dict[graph_name][\"intersection between attacks and victims\"] = len(attackers.intersection(victims))\n",
    "\n",
    "\n",
    "    # Interpretation:\n",
    "    # - Attackers: Source nodes of edges labeled as \"Attack\".\n",
    "    # - Victims: Target nodes of edges labeled as \"Attack\".\n",
    "    # - These metrics provide insight into the roles of nodes in attack scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Number of Attackers: 724\n",
      "multi_di_graph  Number of Victims: 1488\n",
      "multi_di_graph  Proportion of Attackers: 0.00786161814687327\n",
      "multi_di_graph  Proportion of Victims: 0.016157579837772686\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    if g.nx_type == nx.MultiDiGraph:\n",
    "        attackers_victims(g.graph, results, g.name, dataset.label_col)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Metrics Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_clustering_coefficients(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Clustering Coefficient Distribution Metric\n",
    "    clustering_coefficients = nx.clustering(nx.Graph(graph))  # Convert MultiDiGraph to Graph for clustering\n",
    "    clustering_values = list(clustering_coefficients.values())\n",
    "    mean_clustering = np.mean(clustering_values)\n",
    "    std_clustering = np.std(clustering_values)\n",
    "\n",
    "    # print(\"Clustering Coefficients:\", clustering_coefficients)\n",
    "    print(graph_name, \" Mean Clustering Coefficient:\", mean_clustering)\n",
    "    print(graph_name, \" Standard Deviation of Clustering Coefficients:\", std_clustering)\n",
    "\n",
    "    results_dict[graph_name][\"Mean Clustering Coefficient\"] = mean_clustering\n",
    "    results_dict[graph_name][\"Standard Deviation of Clustering Coefficients\"] = std_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Mean Clustering Coefficient: 9.574336153796037e-05\n",
      "multi_di_graph  Standard Deviation of Clustering Coefficients: 0.006364805157974324\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    cal_clustering_coefficients(g.graph, results, g.name)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_degree_assortativity(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Graph Assortativity Metric\n",
    "    try:\n",
    "        degree_assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "        results_dict[graph_name][\"Graph Degree Assortativity Coefficient\"] = degree_assortativity\n",
    "        print(graph_name, \" Degree Assortativity Coefficient:\", degree_assortativity)\n",
    "    except nx.NetworkXError as e:\n",
    "        results_dict[graph_name][\"Graph Degree Assortativity Coefficient\"] = \"not applicable\"\n",
    "        print(graph_name, \" Error calculating assortativity:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Degree Assortativity Coefficient: -0.13294248169616604\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    cal_degree_assortativity(g.graph, results, g.name)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_diameter(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Graph Diameter Metric\n",
    "    try:\n",
    "        if nx.is_strongly_connected(graph):\n",
    "            diameter = nx.diameter(graph)\n",
    "            results_dict[graph_name][\"diameter\"] = diameter\n",
    "            print(graph_name, \" Graph Diameter multidigraph:\", diameter)\n",
    "        else:\n",
    "            results_dict[graph_name][\"diameter\"] = \"not applicable\"\n",
    "            print(graph_name, \" Graph is not strongly connected, diameter is undefined.\")\n",
    "    \n",
    "    except nx.NetworkXError as e:\n",
    "        print(\"Error calculating diameter:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Graph is not strongly connected, diameter is undefined.\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    cal_diameter(g.graph, results, g.name)\n",
    "\n",
    "with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_length_distribution(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Path Length Distribution Metric\n",
    "    try:\n",
    "        path_lengths = dict(nx.all_pairs_shortest_path_length(graph))\n",
    "        all_lengths = [length for source in path_lengths.values() for length in source.values()]\n",
    "        mean_path_length = np.mean(all_lengths)\n",
    "        std_path_length = np.std(all_lengths)\n",
    "\n",
    "        print(graph_name, \" Mean Path Length MultiDiGraph:\", mean_path_length)\n",
    "        print(graph_name, \" Standard Deviation of Path Lengths MultiDiGraph:\", std_path_length)\n",
    "        results_dict[graph_name][\"Mean Path Length\"] = mean_path_length\n",
    "        results_dict[graph_name][\"Standard Deviation of Path Lengths\"] = std_path_length\n",
    "        \n",
    "    except nx.NetworkXError as e:\n",
    "        results_dict[graph_name][\"Mean Path Length\"] = \"not applicable\"\n",
    "        results_dict[graph_name][\"Standard Deviation of Path Lengths\"] = \"not applicable\"\n",
    "        print(graph_name, \" Error calculating path length distribution:\", e)\n",
    "\n",
    "    # Interpretation:\n",
    "    # - Diameter: Longest shortest path in the graph (undefined for disconnected graphs).\n",
    "    # - Assortativity: Correlation of node degrees (positive, negative, or neutral).\n",
    "    # - Clustering Coefficients: Measure of local connectivity (distribution provides network structure insights).\n",
    "    # - Path Lengths: Reachability analysis using shortest paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    if not g.with_ports:\n",
    "        path_length_distribution(g.graph, results, g.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "    f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
