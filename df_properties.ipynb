{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "\n",
    "from src.dataset.dataset_info import datasets\n",
    "from src.numpy_encoder import NumpyEncoder\n",
    "from src.graph.graph_measures import calculate_graph_measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"cic_ton_iot_5_percent\"\n",
    "# name = \"cic_ton_iot\"\n",
    "# name = \"cic_ids_2017_5_percent\"\n",
    "# name = \"cic_ids_2017\"\n",
    "# name = \"cic_bot_iot\"\n",
    "# name = \"cic_ton_iot_modified\"\n",
    "# name = \"nf_ton_iotv2_modified\"\n",
    "# name = \"ccd_inid_modified\"\n",
    "# name = \"nf_uq_nids_modified\"\n",
    "# name = \"edge_iiot\"\n",
    "# name = \"nf_cse_cic_ids2018\"\n",
    "# name = \"nf_bot_iotv2\"\n",
    "# name = \"nf_uq_nids\"\n",
    "name = \"x_iiot\"\n",
    "\n",
    "dataset = datasets[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(dataset.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphsType:\n",
    "    def __init__(self, name, nx_type, graph = None, with_ports = False):\n",
    "        self.name = name\n",
    "        self.nx_type = nx_type\n",
    "        self.graph = graph\n",
    "        self.with_ports = with_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "data_s = \"dataset\"\n",
    "\n",
    "new_file_name = \"df_properties_new.json\"\n",
    "graphs_types = [\n",
    "    GraphsType(\"multi_di_graph\", nx.MultiDiGraph),\n",
    "    # GraphsType(\"multi_di_graph_with_ports\", nx.MultiDiGraph, with_ports = True),\n",
    "    # GraphsType(\"di_graph\", nx.DiGraph),\n",
    "    # GraphsType(\"di_graph_with_ports\", nx.DiGraph, with_ports = True),\n",
    "]\n",
    "\n",
    "results[\"name\"] = name\n",
    "results[data_s] = {}\n",
    "\n",
    "for g in graphs_types:\n",
    "    results[g.name] = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      "class2\n",
      "Normal               324810\n",
      "RDOS                 110007\n",
      "Reconnaissance        99118\n",
      "Weaponization         50569\n",
      "Lateral _movement     23060\n",
      "Exfiltration          17202\n",
      "Tampering              3940\n",
      "C&C                    2216\n",
      "Exploitation            887\n",
      "crypto-ransomware         5\n",
      "Name: count, dtype: int64\n",
      "Class Proportions: [5.14091172e-01 1.74112951e-01 1.56878448e-01 8.00377959e-02\n",
      " 3.64980833e-02 2.72263673e-02 6.23601250e-03 3.50736134e-03\n",
      " 1.40389418e-03 7.91372144e-06]\n",
      "Multi-class Gini Coefficient: 0.6833245227234597\n",
      "Label Counts:\n",
      "class3\n",
      "0    324810\n",
      "1    307004\n",
      "Name: count, dtype: int64\n",
      "Label Proportions: [0.51409117 0.48590883]\n",
      "Binary Classification Gini Coefficient: 0.014091172402004304\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the Gini coefficient\n",
    "def gini_coefficient(values):\n",
    "    sorted_values = np.sort(values)\n",
    "    n = len(sorted_values)\n",
    "    cumulative_values = np.cumsum(sorted_values)\n",
    "    gini = (n + 1 - 2 * np.sum(cumulative_values) / np.sum(sorted_values)) / n\n",
    "    return gini\n",
    "\n",
    "# Multi-class Gini Coefficient\n",
    "class_counts = df[dataset.class_col].value_counts()\n",
    "class_proportions = class_counts.values / class_counts.values.sum()\n",
    "multi_class_gini = gini_coefficient(class_proportions)\n",
    "\n",
    "print(\"Class Counts:\")\n",
    "print(class_counts)\n",
    "print(\"Class Proportions:\", class_proportions)\n",
    "print(\"Multi-class Gini Coefficient:\", multi_class_gini)\n",
    "\n",
    "results[data_s][\"Class Counts / Proportions\"] = {\n",
    "    class_name: {\n",
    "        \"count\": int(count),\n",
    "        \"proportion\": float(proportion)\n",
    "    }\n",
    "    for class_name, count, proportion in zip(class_counts.index, class_counts.values, class_proportions)\n",
    "}\n",
    "# results[data_s][\"Class Proportions\"] = class_proportions\n",
    "\n",
    "# Binary Classification Gini Coefficient\n",
    "label_counts = df[dataset.label_col].value_counts()\n",
    "label_proportions = label_counts.values / label_counts.values.sum()\n",
    "binary_gini = gini_coefficient(label_proportions)\n",
    "\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print(\"Label Proportions:\", label_proportions)\n",
    "print(\"Binary Classification Gini Coefficient:\", binary_gini)\n",
    "\n",
    "\n",
    "results[data_s][\"Multi-class Gini Coefficient\"] = multi_class_gini\n",
    "results[data_s][\"Binary Classification Gini Coefficient\"] = binary_gini\n",
    "\n",
    "total_count = len(df)\n",
    "results[data_s][\"length\"] = total_count\n",
    "num_benign = len(df[df[dataset.label_col] == 0])\n",
    "num_attack = len(df[df[dataset.label_col] == 1])\n",
    "\n",
    "results[data_s][\"num_benign\"] = num_benign\n",
    "results[data_s][\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "results[data_s][\"num_attack\"] = num_attack\n",
    "results[data_s][\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "results[data_s][\"attacks\"] = list(df[dataset.class_col].unique())\n",
    "\n",
    "# Interpretation:\n",
    "# - A Gini coefficient closer to 0 indicates balanced distribution.\n",
    "# - A Gini coefficient closer to 1 indicates imbalanced distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'x_iiot',\n",
       " 'dataset': {'Class Counts / Proportions': {'Normal': {'count': 324810,\n",
       "    'proportion': 0.5140911724020044},\n",
       "   'RDOS': {'count': 110007, 'proportion': 0.17411295096341645},\n",
       "   'Reconnaissance': {'count': 99118, 'proportion': 0.15687844840411894},\n",
       "   'Weaponization': {'count': 50569, 'proportion': 0.08003779593361338},\n",
       "   'Lateral _movement': {'count': 23060, 'proportion': 0.03649808329666642},\n",
       "   'Exfiltration': {'count': 17202, 'proportion': 0.027226367253653766},\n",
       "   'Tampering': {'count': 3940, 'proportion': 0.006236012497348903},\n",
       "   'C&C': {'count': 2216, 'proportion': 0.0035073613436865914},\n",
       "   'Exploitation': {'count': 887, 'proportion': 0.0014038941840478368},\n",
       "   'crypto-ransomware': {'count': 5, 'proportion': 7.913721443336171e-06}},\n",
       "  'Multi-class Gini Coefficient': np.float64(0.6833245227234597),\n",
       "  'Binary Classification Gini Coefficient': np.float64(0.014091172402004304),\n",
       "  'length': 631814,\n",
       "  'num_benign': 324810,\n",
       "  'percentage_of_benign_records': 51.40911724020044,\n",
       "  'num_attack': 307004,\n",
       "  'percentage_of_attack_records': 48.59088275979956,\n",
       "  'attacks': ['Reconnaissance',\n",
       "   'Normal',\n",
       "   'RDOS',\n",
       "   'Weaponization',\n",
       "   'Lateral _movement',\n",
       "   'Exfiltration',\n",
       "   'Exploitation',\n",
       "   'Tampering',\n",
       "   'C&C',\n",
       "   'crypto-ransomware']},\n",
       " 'multi_di_graph': {}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_pairs(df_cs, source_ip, destination_ip, class_column, results_dict, graph_name, folder_path):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    same_class_pairs = {}\n",
    "    mixed_class_pairs = []\n",
    "\n",
    "    # Group by source and destination IP addresses\n",
    "    for (source, destination), group in df_cs.groupby([source_ip, destination_ip]):\n",
    "        unique_classes = group[class_column].unique()\n",
    "        if len(unique_classes) == 1:\n",
    "            # All records have the same class\n",
    "            class_label = str(unique_classes[0])\n",
    "            if class_label not in same_class_pairs:\n",
    "                same_class_pairs[class_label] = []\n",
    "            same_class_pairs[class_label].append({\n",
    "                'node_pair': (source, destination),\n",
    "                'num_instances': len(group)\n",
    "            })\n",
    "        else:\n",
    "            # Mixed class scenario\n",
    "            class_counts = group[class_column].value_counts().to_dict()\n",
    "            total_instances = len(group)\n",
    "            class_percentages = {str(cls): count / total_instances for cls, count in class_counts.items()}\n",
    "            mixed_class_pairs.append({\n",
    "                'node_pair': (source, destination),\n",
    "                'class_counts': class_counts,\n",
    "                'class_percentages': class_percentages\n",
    "            })\n",
    "\n",
    "\n",
    "    # Output results\n",
    "    # print(\"Node pairs with the same class:\")\n",
    "    # for class_label, pairs in same_class_pairs.items():\n",
    "    #     print(f\"Class {class_label}: {pairs}\")\n",
    "\n",
    "    # print(\"\\nNode pairs with mixed classes:\")\n",
    "    # for mixed_pair in mixed_class_pairs:\n",
    "    #     print(mixed_pair)\n",
    "            \n",
    "    if save_to_file:\n",
    "        with open(os.path.join(folder_path, f\"{graph_name}_same_class_pairs.json\"), \"w\") as f:\n",
    "            f.writelines(json.dumps(same_class_pairs, cls=NumpyEncoder))\n",
    "            \n",
    "        with open(os.path.join(folder_path, f\"{graph_name}_mixed_class_pairs.json\"), \"w\") as f:\n",
    "            f.writelines(json.dumps(mixed_class_pairs, cls=NumpyEncoder))\n",
    "\n",
    "    # Total counts\n",
    "    total_same_class_pairs = sum(len(pairs) for pairs in same_class_pairs.values())\n",
    "    total_mixed_class_pairs = len(mixed_class_pairs)\n",
    "\n",
    "    print(\"\\nTotal number of same class pairs:\", total_same_class_pairs)\n",
    "    print(\"Total number of mixed class pairs:\", total_mixed_class_pairs)\n",
    "    \n",
    "    results_dict[graph_name][\"total_same_class_pairs\"] = total_same_class_pairs\n",
    "    results_dict[graph_name][\"total_mixed_class_pairs\"] = total_mixed_class_pairs\n",
    "\n",
    "    # Interpretation:\n",
    "    # - `same_class_pairs` contains node pairs with consistent classes across all records, including the number of instances.\n",
    "    # - `mixed_class_pairs` contains node pairs with mixed classes, the counts and percentages for each class.\n",
    "    # - Total counts provide an overview of the dataset's class consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_classes = os.path.join(\"datasets\", name, \"class_pairs\")\n",
    "\n",
    "for g in graphs_types:\n",
    "    if not g.with_ports:\n",
    "        class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_col, results, g.name, folder_path_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    if not g.with_ports:\n",
    "        g.graph = nx.from_pandas_edgelist(df, dataset.src_ip_col, dataset.dst_ip_col, edge_attr=[dataset.label_col, dataset.class_num_col], create_using=g.nx_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(g.with_ports for g in graphs_types):\n",
    "    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
    "    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n",
    "\n",
    "    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
    "    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    if g.with_ports:\n",
    "        g.graph = nx.from_pandas_edgelist(df, dataset.src_ip_col, dataset.dst_ip_col, edge_attr=[dataset.label_col, dataset.class_num_col], create_using=g.nx_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    if g.with_ports:\n",
    "        class_pairs(df, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_col, results, g.name, folder_path_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "==>> calculated degrees, in 0.00015610083937644958 seconds\n",
      "{'number_of_nodes': 112, 'number_of_edges': 631814, 'max_degree': 294296, 'avg_degree': 11282.392857142857, 'density': 50.82158944658945}\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(g.name)\n",
    "    graph_measures = calculate_graph_measures(g.graph, verbose=True)\n",
    "    \n",
    "    results[g.name][\"graph_measures\"] = graph_measures\n",
    "    print(graph_measures)\n",
    "\n",
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs_types:\n",
    "    results[g.name][\"is_strongly_connected\"] = nx.is_strongly_connected(g.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_centrality_skewness(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Compute degree centrality\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "\n",
    "    # Extract the values of degree centrality\n",
    "    degree_values = list(degree_centrality.values())\n",
    "\n",
    "    # Calculate skewness and kurtosis\n",
    "    degree_skewness = skew(degree_values)\n",
    "    degree_kurtosis = kurtosis(degree_values, fisher=True)  # Fisher=True returns excess kurtosis\n",
    "\n",
    "    print(graph_name, \" Skewness of Degree Centrality:\", degree_skewness)\n",
    "    print(graph_name, \" Kurtosis of Degree Centrality:\", degree_kurtosis)\n",
    "\n",
    "    results_dict[graph_name][\"degree_skewness\"] = degree_skewness\n",
    "    results_dict[graph_name][\"degree_kurtosis\"] = degree_kurtosis\n",
    "    # Interpretation:\n",
    "    # - A high positive skewness indicates a long tail on the right (few nodes with very high centrality).\n",
    "    # - A high kurtosis indicates heavy tails or a highly peaked distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Skewness of Degree Centrality: 174.18771497053797\n",
      "multi_di_graph  Kurtosis of Degree Centrality: 39600.48475147817\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    degree_centrality_skewness(g.graph, results, g.name)\n",
    "\n",
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attackers / Victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attackers_victims(graph, results_dict, graph_name, label_col):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Step 1: Identify unique nodes involved in attack and normal traffic\n",
    "    attackers = set()\n",
    "    victims = set()\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if data[label_col] == 1:\n",
    "            attackers.add(u)\n",
    "            victims.add(v)\n",
    "\n",
    "    # Step 2: Count unique attackers and victims\n",
    "    num_attackers = len(attackers)\n",
    "    num_victims = len(victims)\n",
    "\n",
    "    # Step 3: Calculate proportions\n",
    "    total_nodes = graph.number_of_nodes()\n",
    "    attacker_proportion = num_attackers / total_nodes if total_nodes > 0 else 0\n",
    "    victim_proportion = num_victims / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "    # print(graph_name, \" Unique Attackers:\", attackers)\n",
    "    # print(graph_name, \" Unique Victims:\", victims)\n",
    "    print(graph_name, \" Number of Attackers:\", num_attackers)\n",
    "    print(graph_name, \" Number of Victims:\", num_victims)\n",
    "    print(graph_name, \" Proportion of Attackers:\", attacker_proportion)\n",
    "    print(graph_name, \" Proportion of Victims:\", victim_proportion)\n",
    "\n",
    "    results_dict[graph_name][\"total_nodes\"] = total_nodes\n",
    "    results_dict[graph_name][\"Number of Attackers\"] = num_attackers\n",
    "    results_dict[graph_name][\"Number of Victims\"] = num_victims\n",
    "    results_dict[graph_name][\"Proportion of Attackers\"] = attacker_proportion\n",
    "    results_dict[graph_name][\"Proportion of Victims\"] = victim_proportion\n",
    "    results_dict[graph_name][\"intersection between attacks and victims\"] = len(attackers.intersection(victims))\n",
    "\n",
    "\n",
    "    # Interpretation:\n",
    "    # - Attackers: Source nodes of edges labeled as \"Attack\".\n",
    "    # - Victims: Target nodes of edges labeled as \"Attack\".\n",
    "    # - These metrics provide insight into the roles of nodes in attack scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Number of Attackers: 749\n",
      "multi_di_graph  Number of Victims: 1651\n",
      "multi_di_graph  Proportion of Attackers: 0.007998291419723424\n",
      "multi_di_graph  Proportion of Victims: 0.01763041272892306\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    if g.nx_type == nx.MultiDiGraph:\n",
    "        attackers_victims(g.graph, results, g.name, dataset.label_col)\n",
    "\n",
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Metrics Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_clustering_coefficients(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Clustering Coefficient Distribution Metric\n",
    "    clustering_coefficients = nx.clustering(nx.Graph(graph))  # Convert MultiDiGraph to Graph for clustering\n",
    "    clustering_values = list(clustering_coefficients.values())\n",
    "    mean_clustering = np.mean(clustering_values)\n",
    "    std_clustering = np.std(clustering_values)\n",
    "\n",
    "    # print(\"Clustering Coefficients:\", clustering_coefficients)\n",
    "    print(graph_name, \" Mean Clustering Coefficient:\", mean_clustering)\n",
    "    print(graph_name, \" Standard Deviation of Clustering Coefficients:\", std_clustering)\n",
    "\n",
    "    results_dict[graph_name][\"Mean Clustering Coefficient\"] = mean_clustering\n",
    "    results_dict[graph_name][\"Standard Deviation of Clustering Coefficients\"] = std_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Mean Clustering Coefficient: 0.0006704149168668995\n",
      "multi_di_graph  Standard Deviation of Clustering Coefficients: 0.02265764720880905\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    cal_clustering_coefficients(g.graph, results, g.name)\n",
    "\n",
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_degree_assortativity(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Graph Assortativity Metric\n",
    "    try:\n",
    "        degree_assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "        results_dict[graph_name][\"Graph Degree Assortativity Coefficient\"] = degree_assortativity\n",
    "        print(graph_name, \" Degree Assortativity Coefficient:\", degree_assortativity)\n",
    "    except nx.NetworkXError as e:\n",
    "        results_dict[graph_name][\"Graph Degree Assortativity Coefficient\"] = \"not applicable\"\n",
    "        print(graph_name, \" Error calculating assortativity:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Degree Assortativity Coefficient: -0.25629570112028727\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    cal_degree_assortativity(g.graph, results, g.name)\n",
    "\n",
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_diameter(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Graph Diameter Metric\n",
    "    try:\n",
    "        if nx.is_strongly_connected(graph):\n",
    "            diameter = nx.diameter(graph)\n",
    "            results_dict[graph_name][\"diameter\"] = diameter\n",
    "            print(graph_name, \" Graph Diameter multidigraph:\", diameter)\n",
    "        else:\n",
    "            results_dict[graph_name][\"diameter\"] = \"not applicable\"\n",
    "            print(graph_name, \" Graph is not strongly connected, diameter is undefined.\")\n",
    "    \n",
    "    except nx.NetworkXError as e:\n",
    "        print(\"Error calculating diameter:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n",
      "multi_di_graph  Graph is not strongly connected, diameter is undefined.\n"
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    cal_diameter(g.graph, results, g.name)\n",
    "\n",
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_length_distribution(graph, results_dict, graph_name):\n",
    "    print(\"====================\")\n",
    "    print(\"====================\")\n",
    "    print(graph_name)\n",
    "    # Path Length Distribution Metric\n",
    "    try:\n",
    "        path_lengths = dict(nx.all_pairs_shortest_path_length(graph))\n",
    "        all_lengths = [length for source in path_lengths.values() for length in source.values()]\n",
    "        mean_path_length = np.mean(all_lengths)\n",
    "        std_path_length = np.std(all_lengths)\n",
    "\n",
    "        print(graph_name, \" Mean Path Length MultiDiGraph:\", mean_path_length)\n",
    "        print(graph_name, \" Standard Deviation of Path Lengths MultiDiGraph:\", std_path_length)\n",
    "        results_dict[graph_name][\"Mean Path Length\"] = mean_path_length\n",
    "        results_dict[graph_name][\"Standard Deviation of Path Lengths\"] = std_path_length\n",
    "        \n",
    "    except nx.NetworkXError as e:\n",
    "        results_dict[graph_name][\"Mean Path Length\"] = \"not applicable\"\n",
    "        results_dict[graph_name][\"Standard Deviation of Path Lengths\"] = \"not applicable\"\n",
    "        print(graph_name, \" Error calculating path length distribution:\", e)\n",
    "\n",
    "    # Interpretation:\n",
    "    # - Diameter: Longest shortest path in the graph (undefined for disconnected graphs).\n",
    "    # - Assortativity: Correlation of node degrees (positive, negative, or neutral).\n",
    "    # - Clustering Coefficients: Measure of local connectivity (distribution provides network structure insights).\n",
    "    # - Path Lengths: Reachability analysis using shortest paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "multi_di_graph\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs_types:\n\u001b[0;32m      2\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m g\u001b[39m.\u001b[39mwith_ports:\n\u001b[1;32m----> 3\u001b[0m         path_length_distribution(g\u001b[39m.\u001b[39;49mgraph, results, g\u001b[39m.\u001b[39;49mname)\n",
      "Cell \u001b[1;32mIn[52], line 7\u001b[0m, in \u001b[0;36mpath_length_distribution\u001b[1;34m(graph, results_dict, graph_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Path Length Distribution Metric\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     path_lengths \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(nx\u001b[39m.\u001b[39;49mall_pairs_shortest_path_length(graph))\n\u001b[0;32m      8\u001b[0m     all_lengths \u001b[39m=\u001b[39m [length \u001b[39mfor\u001b[39;00m source \u001b[39min\u001b[39;00m path_lengths\u001b[39m.\u001b[39mvalues() \u001b[39mfor\u001b[39;00m length \u001b[39min\u001b[39;00m source\u001b[39m.\u001b[39mvalues()]\n\u001b[0;32m      9\u001b[0m     mean_path_length \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(all_lengths)\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\GNN-NIDS\\.venv\\Lib\\site-packages\\networkx\\algorithms\\shortest_paths\\unweighted.py:199\u001b[0m, in \u001b[0;36mall_pairs_shortest_path_length\u001b[1;34m(G, cutoff)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39m# TODO This can be trivially parallelized.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m G:\n\u001b[1;32m--> 199\u001b[0m     \u001b[39myield\u001b[39;00m (n, length(G, n, cutoff\u001b[39m=\u001b[39;49mcutoff))\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 62:3\u001b[0m, in \u001b[0;36margmap_single_source_shortest_path_length_59\u001b[1;34m(G, source, cutoff, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\GNN-NIDS\\.venv\\Lib\\site-packages\\networkx\\utils\\backends.py:633\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the result of the original function, or the backend function if\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39mthe backend is specified and that backend implements `func`.\"\"\"\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m backends:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# Fast path if no backends are installed\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    635\u001b[0m \u001b[39m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[0;32m    636\u001b[0m backend_name \u001b[39m=\u001b[39m backend\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\GNN-NIDS\\.venv\\Lib\\site-packages\\networkx\\algorithms\\shortest_paths\\unweighted.py:62\u001b[0m, in \u001b[0;36msingle_source_shortest_path_length\u001b[1;34m(G, source, cutoff)\u001b[0m\n\u001b[0;32m     60\u001b[0m     cutoff \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m nextlevel \u001b[39m=\u001b[39m [source]\n\u001b[1;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(_single_shortest_path_length(G\u001b[39m.\u001b[39m_adj, nextlevel, cutoff))\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\GNN-NIDS\\.venv\\Lib\\site-packages\\networkx\\algorithms\\shortest_paths\\unweighted.py:-1\u001b[0m, in \u001b[0;36m_single_shortest_path_length\u001b[1;34m(adj, firstlevel, cutoff)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for g in graphs_types:\n",
    "    if not g.with_ports:\n",
    "        path_length_distribution(g.graph, results, g.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_to_file:\n",
    "    with open(os.path.join(\"datasets\", name, new_file_name), \"w\") as f:\n",
    "        f.writelines(json.dumps(results, cls=NumpyEncoder))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
