{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-24T12:01:21.407444Z","iopub.status.busy":"2024-08-24T12:01:21.407010Z","iopub.status.idle":"2024-08-24T12:01:29.827271Z","shell.execute_reply":"2024-08-24T12:01:29.826059Z","shell.execute_reply.started":"2024-08-24T12:01:21.407404Z"},"trusted":true},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 1\n","\n","import json\n","import os\n","import pickle\n","import time\n","import timeit\n","import random\n","\n","import numpy as np\n","\n","os.environ[\"DGLBACKEND\"] = \"pytorch\"\n","\n","import torch as th\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from dgl import from_networkx, edge_subgraph\n","from dgl.nn.pytorch import EdgeWeightNorm\n","from sklearn.metrics import (\n","    classification_report,\n","    confusion_matrix,\n",")\n","from sklearn.utils import class_weight\n","\n","from src.calculate_FPR_FNR import calculate_FPR_FNR_with_global\n","from src.dataset.dataset_info import datasets\n","import src.models as models\n","# from src.models import EGAT, EGCN, EGRAPHSAGE, Model\n","from src.plot_confusion_matrix import plot_confusion_matrix\n","from src.numpy_encoder import NumpyEncoder\n","\n","seed = 42  # or any constant value\n","random.seed(seed)\n","np.random.seed(seed)\n","th.manual_seed(seed)\n","\n","%aimport src.models\n","\n","num_epochs = 200\n","batch_size = 128\n","learning_rate = 0.005\n","LAMBD_2 = 0.001"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["'datasets\\\\nf_cse_cic_ids2018'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# name = \"cic_ton_iot_5_percent\"\n","# name = \"cic_ton_iot\"\n","# name = \"cic_ids_2017_5_percent\"\n","# name = \"cic_ids_2017\"\n","# name = \"cic_bot_iot\"\n","# name = \"cic_ton_iot_modified\"\n","# name = \"nf_ton_iotv2_modified\"\n","# name = \"ccd_inid_modified\"\n","# name = \"nf_uq_nids_modified\"\n","# name = \"edge_iiot\"\n","name = \"nf_cse_cic_ids2018\"\n","# name = \"nf_bot_iotv2\"\n","# name = \"nf_uq_nids\"\n","# name = \"x_iiot\"\n","\n","use_node_features = False\n","node_features_version = 1\n","\n","using_masking = False\n","masked_class = 2\n","\n","multi_class = True\n","\n","# dataset properties\n","use_port_in_address = False\n","generated_ips = False\n","\n","graph_type = \"flow\"\n","\n","sort_timestamp = False\n","\n","dataset = datasets[name]\n","\n","dataset_folder = os.path.join(\"datasets\", name)\n","dataset_folder"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["'datasets\\\\nf_cse_cic_ids2018\\\\flow__multi_class__unsorted'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["g_type = \"flow\"\n","    \n","if multi_class:\n","    g_type += \"__multi_class\"\n","    \n","if use_node_features:\n","    g_type += \"__n_feats\"\n","    \n","# if k_fold:\n","#     g_type += f\"__{k_fold}_fold\"\n","    \n","if use_port_in_address:\n","    g_type += \"__ports\"\n","    \n","if generated_ips:\n","    g_type += \"__generated_ips\"\n","    \n","if sort_timestamp:\n","    g_type += \"__sorted\"\n","else:\n","    g_type += \"__unsorted\"\n","    \n","graphs_folder = os.path.join(dataset_folder, g_type)\n","graphs_folder"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["number_neighbors = [25, 10]\n","# number_neighbors = None\n","num_layers=2\n","ndim_out = [128, 128]\n","aggregation=\"mean\"\n","# aggregation=\"pool\"\n","# aggregation=\"lstm\"\n","# aggregation=\"gcn\"\n","activation=F.relu\n","dropout=0.2\n","\n","my_models = [\n","    # models.Model(\"e_gcn\", models.EGCN, num_layers=num_layers, ndim_out= ndim_out, activation=activation, dropout=dropout, residual=False, norm=False),\n","    models.Model(\"e_gcn_res\", models.EGCN, num_layers=num_layers, ndim_out= ndim_out, activation=activation, dropout=dropout, residual=True, norm=False),\n","    # models.Model(\"e_graph_sage\", models.EGRAPHSAGE, num_layers=num_layers, ndim_out= ndim_out, activation=activation, dropout=dropout, residual=False, aggregation=aggregation, num_neighbors=number_neighbors),\n","    models.Model(\"e_graph_sage_res\", models.EGRAPHSAGE, num_layers=num_layers, ndim_out= ndim_out, activation=activation, dropout=dropout, residual=True, aggregation=aggregation, num_neighbors=number_neighbors),\n","    # models.Model(\"e_gat\", models.EGAT, num_layers=num_layers, ndim_out= ndim_out, activation=activation, dropout=dropout, residual=False),\n","    models.Model(\"e_gat_res\", models.EGAT, num_layers=num_layers, ndim_out= ndim_out, activation=activation, dropout=dropout, residual=True),\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["{'name': 'nf_cse_cic_ids2018',\n"," 'g_type': 'flow__multi_class__unsorted',\n"," 'configuration': {'num_epochs': 200,\n","  'multi_class': True,\n","  'batch_size': 128,\n","  'learning_rate': 0.005,\n","  'num_neighbors': [25, 10],\n","  'use_node_features': False,\n","  'node_features_version': 1,\n","  'using_masking': False,\n","  'masked_class_num': 2,\n","  'e_graph_sage_aggregation': 'mean',\n","  'LAMBD_2': 0.001},\n"," 'accuracy': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'f1_score': {},\n"," 'FPR': {},\n"," 'FNR': {},\n"," 'time_elapsed': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'train_accuracy': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'train_loss': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_accuracy': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_loss': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_precision': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_recall': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_f1': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_FPR': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'val_FNR': {'e_gcn_res': [], 'e_graph_sage_res': [], 'e_gat_res': []},\n"," 'e_gcn_res': {},\n"," 'e_graph_sage_res': {},\n"," 'e_gat_res': {}}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["results_final = {}\n","\n","results_final[\"name\"] = name\n","results_final[\"g_type\"] = g_type\n","results_final[\"configuration\"] = {\n","    \"num_epochs\": num_epochs,\n","    \"multi_class\": multi_class,\n","    \"batch_size\": batch_size,\n","    \"learning_rate\": learning_rate,\n","    \"num_neighbors\": number_neighbors,\n","    \"use_node_features\": use_node_features,\n","    \"node_features_version\": node_features_version,\n","    \"using_masking\": using_masking,\n","    \"masked_class_num\": masked_class,\n","    \"e_graph_sage_aggregation\": aggregation,\n","    \"LAMBD_2\": LAMBD_2,\n","}\n","\n","results_final[\"accuracy\"] = {}\n","results_final[\"f1_score\"] = {}\n","results_final[\"FPR\"] = {}\n","results_final[\"FNR\"] = {}\n","results_final[\"time_elapsed\"] = {}\n","results_final[\"train_accuracy\"] = {}\n","results_final[\"train_loss\"] = {}\n","results_final[\"val_accuracy\"] = {}\n","results_final[\"val_loss\"] = {}\n","results_final[\"val_precision\"] = {}\n","results_final[\"val_recall\"] = {}\n","results_final[\"val_f1\"] = {}\n","results_final[\"val_FPR\"] = {}\n","results_final[\"val_FNR\"] = {}\n","\n","for m in my_models:\n","    results_final[m.model_name] = {}\n","    results_final[\"accuracy\"][m.model_name] = []\n","    results_final[\"time_elapsed\"][m.model_name] = []\n","    results_final[\"train_accuracy\"][m.model_name] = []\n","    results_final[\"train_loss\"][m.model_name] = []\n","    results_final[\"val_accuracy\"][m.model_name] = []\n","    results_final[\"val_loss\"][m.model_name] = []\n","    results_final[\"val_precision\"][m.model_name] = []\n","    results_final[\"val_recall\"][m.model_name] = []\n","    results_final[\"val_f1\"][m.model_name] = []\n","    results_final[\"val_FPR\"][m.model_name] = []\n","    results_final[\"val_FNR\"][m.model_name] = []\n","\n","results_final"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'20250130-142144'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n","dtime"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["results_folder_path = \"results\"\n","results_folder_path1 = os.path.join(results_folder_path, name)\n","results_folder_path2 = os.path.join(results_folder_path1, g_type)\n","folder_path = os.path.join(results_folder_path2, dtime)\n","confusion_matrices_path = os.path.join(folder_path, \"confusion_matrices\")\n","os.makedirs(confusion_matrices_path, exist_ok=True)\n","os.makedirs(\"temp\", exist_ok=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["([np.str_('Benign'),\n","  np.str_('Bot'),\n","  np.str_('Brute Force -Web'),\n","  np.str_('Brute Force -XSS'),\n","  np.str_('DDOS attack-HOIC'),\n","  np.str_('DDOS attack-LOIC-UDP'),\n","  np.str_('DDoS attacks-LOIC-HTTP'),\n","  np.str_('DoS attacks-GoldenEye'),\n","  np.str_('DoS attacks-Hulk'),\n","  np.str_('DoS attacks-SlowHTTPTest'),\n","  np.str_('DoS attacks-Slowloris'),\n","  np.str_('FTP-BruteForce'),\n","  np.str_('Infilteration'),\n","  np.str_('SQL Injection'),\n","  np.str_('SSH-Bruteforce')],\n"," 15)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["labels = [\"Normal\", \"Attack\"]\n","num_classes = 2\n","if multi_class:\n","    with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n","        labels_names = pickle.load(f)\n","    labels_mapping = labels_names[0]\n","    # labels = labels_names[1]\n","    labels = list(labels_mapping.values())\n","    num_classes = len(labels)\n","labels, num_classes"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["if using_masking:\n","    results_final[\"configuration\"][\"masked_class_name\"] = str(labels[masked_class])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["with open(os.path.join(graphs_folder, \"training_graph.pkl\"), \"rb\") as f:\n","    G = pickle.load(f)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["with open(os.path.join(graphs_folder, \"validation_graph.pkl\"), \"rb\") as f:\n","    G_val = pickle.load(f)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["with open(os.path.join(graphs_folder, \"testing_graph.pkl\"), \"rb\") as f:\n","    G_test = pickle.load(f)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:02:22.712060Z","iopub.status.busy":"2024-08-24T12:02:22.711696Z","iopub.status.idle":"2024-08-24T12:03:18.648324Z","shell.execute_reply":"2024-08-24T12:03:18.646671Z","shell.execute_reply.started":"2024-08-24T12:02:22.712027Z"},"trusted":true},"outputs":[],"source":["edge_attributes = edge_attrs = ['h', dataset.label_col, dataset.class_num_col]\n","\n","if use_node_features:\n","    G = from_networkx(G, edge_attrs=edge_attributes, node_attrs=[\"n_feats\"])\n","    G_val = from_networkx(G_val, edge_attrs=edge_attributes, node_attrs=[\"n_feats\"])  \n","    G_test = from_networkx(G_test, edge_attrs=edge_attributes, node_attrs=[\"n_feats\"])  \n","else:\n","    G = from_networkx(G,  edge_attrs=edge_attributes)\n","    G_val = from_networkx(G_val,  edge_attrs=edge_attributes)\n","    G_test = from_networkx(G_test,  edge_attrs=edge_attributes)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:03:18.650765Z","iopub.status.busy":"2024-08-24T12:03:18.650221Z","iopub.status.idle":"2024-08-24T12:03:18.657333Z","shell.execute_reply":"2024-08-24T12:03:18.656030Z","shell.execute_reply.started":"2024-08-24T12:03:18.650716Z"},"trusted":true},"outputs":[{"data":{"text/plain":["6"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["num_features = G.edata['h'].shape[1]\n","num_features"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["if using_masking:\n","    # Create masks for edges\n","    training_mask = G.edata[dataset.class_num_col] != masked_class  # Include all edges except class 3\n","    # val_mask = G_val.edata[dataset.class_num_col] == masked_class    # Include only edges of class 3 (or other validation logic)\n","    # test_mask = G_test.edata[dataset.class_num_col] == masked_class   # Include only edges of class 3\n","    \n","    G = edge_subgraph(G, training_mask)\n","    # G_val = edge_subgraph(G_val, val_mask)\n","    # G_test = edge_subgraph(G_test, test_mask)\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# if use_node_features:\n","#     from sklearn.preprocessing import StandardScaler\n","#     scaler = StandardScaler()\n","#     device = th.device(\"cpu\")\n","#     scaled_feats = scaler.fit_transform(G.ndata[\"n_feats\"])\n","#     G.ndata[\"n_feats\"] = th.tensor(scaled_feats, device=device, dtype=th.float32)\n","\n","#     # Similarly, transform the validation and test features and convert them\n","#     scaled_feats_val = scaler.transform(G_val.ndata[\"n_feats\"])\n","#     G_val.ndata[\"n_feats\"] = th.tensor(scaled_feats_val, device=device, dtype=th.float32)\n","\n","#     scaled_feats_test = scaler.transform(G_test.ndata[\"n_feats\"])\n","#     G_test.ndata[\"n_feats\"] = th.tensor(scaled_feats_test, device=device, dtype=th.float32)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:03:18.659124Z","iopub.status.busy":"2024-08-24T12:03:18.658752Z","iopub.status.idle":"2024-08-24T12:03:18.690895Z","shell.execute_reply":"2024-08-24T12:03:18.689736Z","shell.execute_reply.started":"2024-08-24T12:03:18.659072Z"},"trusted":true},"outputs":[],"source":["if use_node_features:\n","    # G.ndata[\"h\"] = th.cat([G.ndata[\"n_feats\"], th.ones(G.num_nodes(), num_features)], dim=1)\n","    G.ndata[\"h\"] = G.ndata[\"n_feats\"]\n","else:\n","    G.ndata['h'] = th.ones(G.num_nodes(), num_features)  # noqa: F821\n","    \n","ndim_in = G.ndata[\"h\"].shape[-1]\n","\n","G.ndata['h'] = th.reshape(G.ndata['h'], (G.ndata['h'].shape[0], 1, G.ndata['h'].shape[1]))\n","# G.ndata['h'] = th.reshape(G.ndata['h'], (G.ndata['h'].shape[0], 1, ndim_in))\n","G.edata['h'] = th.reshape(G.edata['h'], (G.edata['h'].shape[0], 1, num_features))\n","\n","G.edata['train_mask'] = th.ones(len(G.edata['h']), dtype=th.bool)\n","# G.edata['train_mask'] = training_mask"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:03:18.759982Z","iopub.status.busy":"2024-08-24T12:03:18.759441Z","iopub.status.idle":"2024-08-24T12:03:19.027970Z","shell.execute_reply":"2024-08-24T12:03:19.026893Z","shell.execute_reply.started":"2024-08-24T12:03:18.759945Z"},"trusted":true},"outputs":[],"source":["if multi_class:\n","    class_weights = class_weight.compute_class_weight('balanced',\n","                                                classes=np.unique(\n","                                                    G.edata[dataset.class_num_col].cpu().numpy()),\n","                                                y=G.edata[dataset.class_num_col].cpu().numpy())\n","else:\n","    class_weights = class_weight.compute_class_weight('balanced',\n","                                                    classes=np.unique(\n","                                                        G.edata[dataset.label_col].cpu().numpy()),\n","                                                    y=G.edata[dataset.label_col].cpu().numpy())"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["if using_masking:\n","    class_weights=np.insert(class_weights, masked_class, 0)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:03:19.029947Z","iopub.status.busy":"2024-08-24T12:03:19.029509Z","iopub.status.idle":"2024-08-24T12:03:19.046918Z","shell.execute_reply":"2024-08-24T12:03:19.045391Z","shell.execute_reply.started":"2024-08-24T12:03:19.029904Z"},"trusted":true},"outputs":[],"source":["class_weights = th.FloatTensor(class_weights)\n","\n","criterion = nn.CrossEntropyLoss(weight=class_weights)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:03:19.053163Z","iopub.status.busy":"2024-08-24T12:03:19.052745Z","iopub.status.idle":"2024-08-24T12:03:19.060739Z","shell.execute_reply":"2024-08-24T12:03:19.059331Z","shell.execute_reply.started":"2024-08-24T12:03:19.053117Z"},"trusted":true},"outputs":[],"source":["def compute_accuracy(pred, labels):\n","    return (pred.argmax(1) == labels).float().mean().item()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["if multi_class:\n","    val_labels = G_val.edata[dataset.class_num_col]\n","else:\n","    val_labels = G_val.edata[dataset.label_col]\n","\n","if use_node_features:\n","    # G_val.ndata[\"feature\"] = th.cat([G_val.ndata[\"n_feats\"], th.ones(G_val.num_nodes(), num_features)], dim=1)\n","    G_val.ndata[\"feature\"] = G_val.ndata[\"n_feats\"]\n","else:\n","    G_val.ndata['feature'] = th.ones(G_val.num_nodes(),  num_features)\n","\n","G_val.edata['val_mask'] = th.ones(len(G_val.edata['h']), dtype=th.bool)\n","# G_val.edata['val_mask'] = val_mask"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["G_val.ndata['feature'] = th.reshape(G_val.ndata['feature'], (G_val.ndata['feature'].shape[0], 1, G_val.ndata['feature'].shape[1]))\n","G_val.edata['h'] = th.reshape(G_val.edata['h'], (G_val.edata['h'].shape[0], 1, G_val.edata['h'].shape[1]))"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["if multi_class:\n","    test_labels = G_test.edata[dataset.class_num_col]\n","else:\n","    test_labels = G_test.edata[dataset.label_col]\n","\n","if use_node_features:\n","    # G_test.ndata[\"feature\"] = th.cat([G_test.ndata[\"n_feats\"], th.ones(G_test.num_nodes(), num_features)], dim=1)\n","    G_test.ndata[\"feature\"] = G_test.ndata[\"n_feats\"]\n","else:\n","    G_test.ndata['feature'] = th.ones(G_test.num_nodes(),  num_features)\n","\n","G_test.edata['test_mask'] = th.ones(len(G_test.edata['h']), dtype=th.bool)\n","# G_test.edata['test_mask'] = test_mask"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":["G_test.ndata['feature'] = th.reshape(G_test.ndata['feature'], (G_test.ndata['feature'].shape[0], 1, G_test.ndata['feature'].shape[1]))\n","G_test.edata['h'] = th.reshape(G_test.edata['h'], (G_test.edata['h'].shape[0], 1, G_test.edata['h'].shape[1]))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def evaluate_model(model:models.Model, graph, actual_labels, loss_fn, results_f, _labels):\n","    start_time = timeit.default_timer()\n","    model.trained_model.eval()\n","\n","    if model.norm:\n","        edge_weight = th.ones(graph.num_edges(), dtype=th.float32)\n","        norm = EdgeWeightNorm(norm='both')\n","        norm_edge_weight = norm(graph, edge_weight)\n","        graph.edata['norm_weight'] = norm_edge_weight\n","\n","    node_features_test = graph.ndata['feature']\n","    edge_features_test = graph.edata['h']\n","    \n","    with th.no_grad():\n","        test_pred = model.trained_model(graph, node_features_test, edge_features_test)\n","        \n","    elapsed = timeit.default_timer() - start_time\n","\n","    loss = loss_fn(test_pred, actual_labels)\n","    \n","    test_pred = test_pred.argmax(1)\n","    test_pred = th.Tensor.cpu(test_pred).detach().numpy()\n","\n","    if multi_class:\n","        actual = np.vectorize(labels_names[0].get)(actual_labels)\n","        test_pred = np.vectorize(labels_names[0].get)(test_pred)\n","    else:\n","        actual = [\"Normal\" if i == 0 else \"Attack\" for i in actual_labels]\n","        test_pred = [\"Normal\" if i == 0 else \"Attack\" for i in test_pred]\n","\n","    cr = classification_report(actual, test_pred, digits=4, output_dict=True, zero_division=0)\n","    cm = confusion_matrix(actual, test_pred, labels=_labels)\n","    results_fpr_fnr = calculate_FPR_FNR_with_global(cm)\n","    \n","    val_acc = cr[\"accuracy\"] * 100\n","    val_f1 = cr['weighted avg']['f1-score'] * 100\n","    val_loss = loss.item()\n","    results_f[\"val_accuracy\"][model.model_name].append(val_acc)\n","    results_f[\"val_loss\"][model.model_name].append(val_loss)\n","    results_f[\"val_precision\"][model.model_name].append(cr['weighted avg']['precision'] * 100)\n","    results_f[\"val_recall\"][model.model_name].append(cr['weighted avg']['recall'] * 100)\n","    results_f[\"val_f1\"][model.model_name].append(val_f1)\n","    results_f[\"val_FPR\"][model.model_name].append(results_fpr_fnr[\"global\"][\"FPR\"])\n","    results_f[\"val_FNR\"][model.model_name].append(results_fpr_fnr[\"global\"][\"FNR\"])\n","    \n","    return (val_acc, val_loss, val_f1, elapsed)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def train_model(model: models.Model, graph, _labels):\n","    node_features = graph.ndata['h']\n","    edge_features = graph.edata['h']\n","\n","    edge_label = graph.edata[dataset.class_num_col if multi_class else dataset.label_col]\n","        \n","    train_mask = graph.edata['train_mask']\n","\n","    # model = EGRAPHSAGE(num_features, num_features, 128, F.relu,\n","    #                    dropout=0.2, num_neighbors=4, residual=residual)\n","\n","    if model.norm:\n","        edge_weight = th.ones(graph.num_edges(), dtype=th.float32)\n","        norm = EdgeWeightNorm(norm='both')\n","        norm_edge_weight = norm(graph, edge_weight)\n","        graph.edata['norm_weight'] = norm_edge_weight\n","\n","    if model.model_class == models.EGRAPHSAGE:\n","        model.trained_model = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation, aggregation=model.aggregation,\n","                            dropout=model.dropout, num_neighbors=model.num_neighbors, residual=model.residual, num_class=num_classes)\n","        model.best_model = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation, aggregation=model.aggregation,\n","                            dropout=model.dropout, num_neighbors=model.num_neighbors, residual=model.residual, num_class=num_classes) \n","        model.best_model_loss = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation, aggregation=model.aggregation,\n","                            dropout=model.dropout, num_neighbors=model.num_neighbors, residual=model.residual, num_class=num_classes) \n","        model.best_model_acc = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation, aggregation=model.aggregation,\n","                            dropout=model.dropout, num_neighbors=model.num_neighbors, residual=model.residual, num_class=num_classes)  \n","    elif model.model_class == models.EGCN:\n","        model.trained_model = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes, norm=model.norm)\n","        model.best_model = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes, norm=model.norm)\n","        model.best_model_loss = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes, norm=model.norm)\n","        model.best_model_acc = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes, norm=model.norm)\n","    else:\n","        model.trained_model = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes)\n","        model.best_model = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes)\n","        model.best_model_loss = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes)\n","        model.best_model_acc = model.model_class(ndim_in, num_features, model.ndim_out, num_layers=model.num_layers, activation=model.activation,\n","                            dropout=model.dropout, residual=model.residual, num_class=num_classes)\n","\n","    opt = th.optim.Adam(model.trained_model.parameters(), lr = learning_rate, weight_decay=LAMBD_2)\n","    \n","    best_f1 = 0\n","    best_acc = 0\n","    best_loss = np.inf\n","    best_f1_epoch = 0\n","    best_acc_epoch = 0\n","    best_loss_epoch = 0\n","    for epoch in range(1, num_epochs):\n","        model.trained_model.train()\n","        pred = model.trained_model(graph, node_features, edge_features[train_mask])\n","        loss = criterion(pred[train_mask], edge_label[train_mask])\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        if epoch == 1:\n","            print(\"================================\")\n","            print(\"================================\")\n","            print(f\"Training Model: {model.model_name}\")\n","            print(f\"Edge label shape: {edge_label.shape}\")\n","            print(f\"Edge label unique values: {th.unique(edge_label)}\")\n","            print(f\"Pred shape: {pred.shape}\")\n","            \n","        train_acc = compute_accuracy(pred[train_mask], edge_label[train_mask]) * 100\n","\n","        train_pred = pred[train_mask].argmax(1)\n","        train_pred = th.Tensor.cpu(train_pred).detach().numpy()\n","\n","        if multi_class:\n","            actual = np.vectorize(labels_names[0].get)(edge_label[train_mask])\n","            train_pred = np.vectorize(labels_names[0].get)(train_pred)\n","        else:\n","            actual = [\"Normal\" if i == 0 else \"Attack\" for i in edge_label[train_mask]]\n","            train_pred = [\"Normal\" if i == 0 else \"Attack\" for i in train_pred]\n","        cr = classification_report(actual, train_pred, digits=4, output_dict=True, zero_division=0)\n","        train_f1 = cr['weighted avg']['f1-score'] * 100   \n","\n","        print(f\"Model: {model.model_name} -- Epoch: {epoch} -- Training acc: {train_acc:.2f}  -- Training f1: {train_f1:.2f} -- Training loss: {loss.item():.4f}\")\n","        \n","        results_final[\"train_accuracy\"][model.model_name].append(train_acc)\n","        results_final[\"train_loss\"][model.model_name].append(loss.item())\n","        \n","        val_acc, val_loss, val_f1, elapsed = evaluate_model(model, G_val, val_labels, criterion, results_final, _labels)\n","\n","        print(f\"Model: {model.model_name} -- Epoch: {epoch} -- Validation acc: {val_acc:.2f} -- Validation f1: {val_f1:.2f} -- Validation loss: {val_loss:.4f}\")\n","        print(\"Time for validation: \", str(elapsed) + ' seconds')    \n","        \n","        if best_f1 < val_f1:\n","            best_f1_epoch = epoch\n","            best_f1 = val_f1\n","            best_model_state = model.trained_model.state_dict().copy()\n","            # th.save(best_model_state, f\"temp/best_model_{model.model_name}.pth\")\n","            th.save(model.trained_model, f\"temp/best_model_{model.model_name}.pth\")\n","\n","        if best_acc < val_acc:\n","            best_acc_epoch = epoch\n","            best_acc = val_acc\n","            model.best_model_acc.load_state_dict(model.trained_model.state_dict().copy())\n","\n","        if best_loss > val_loss:\n","            best_loss_epoch = epoch\n","            best_loss = val_loss\n","            model.best_model_loss.load_state_dict(model.trained_model.state_dict().copy())\n","            \n","    print(f\"==>> best_f1: {best_f1} at epoch: {best_f1_epoch}\")\n","    print(f\"==>> best_acc: {best_acc} at epoch: {best_acc_epoch}\")\n","    print(f\"==>> best_loss: {best_loss} at epoch: {best_loss_epoch}\")\n","    model.best_model.load_state_dict(best_model_state)\n","        \n","    return model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def test_model(model, model_name, norm, graph, actual_labels, results_f, _labels):\n","    print(\"=======================\")\n","    print(f\"testing model: {model_name}\")\n","    \n","    start_time = timeit.default_timer()\n","    model.eval()\n","    \n","    if norm:\n","        edge_weight = th.ones(graph.num_edges(), dtype=th.float32)\n","        norm = EdgeWeightNorm(norm='both')\n","        norm_edge_weight = norm(graph, edge_weight)\n","        graph.edata['norm_weight'] = norm_edge_weight\n","    \n","    node_features_test = graph.ndata['feature']\n","    edge_features_test = graph.edata['h']\n","    \n","    with th.no_grad():\n","        test_pred = model(graph, node_features_test, edge_features_test)\n","        \n","    elapsed = timeit.default_timer() - start_time\n","    \n","    test_pred = test_pred.argmax(1)\n","    test_pred = th.Tensor.cpu(test_pred).detach().numpy()\n","    \n","    if multi_class:\n","        actual = np.vectorize(labels_names[0].get)(actual_labels)\n","        test_pred = np.vectorize(labels_names[0].get)(test_pred)\n","    else:\n","        actual = [\"Normal\" if i == 0 else \"Attack\" for i in actual_labels]\n","        test_pred = [\"Normal\" if i == 0 else \"Attack\" for i in test_pred]\n","\n","    cr = classification_report(actual, test_pred, digits=4, output_dict=True, zero_division=0)\n","    cm = confusion_matrix(actual, test_pred, labels=_labels)\n","    cm_normalized = confusion_matrix(actual, test_pred, labels=labels, normalize=\"true\")\n","    results_fpr_fnr = calculate_FPR_FNR_with_global(cm)\n","\n","    # Log metrics\n","    results_f[model_name][\"elapsed\"] = elapsed\n","    results_f[model_name][\"classification_report\"] = cr\n","    results_f[model_name][\"results_fpr_fnr\"] = results_fpr_fnr\n","    results_f[\"accuracy\"][model_name] = cr[\"accuracy\"] * 100\n","    results_f[\"f1_score\"][model_name] = cr['weighted avg']['f1-score'] * 100\n","    results_f[\"FPR\"][model_name] = results_fpr_fnr[\"global\"][\"FPR\"]\n","    results_f[\"FNR\"][model_name] = results_fpr_fnr[\"global\"][\"FNR\"]\n","    results_f[\"time_elapsed\"][model_name] = elapsed\n","\n","    print(classification_report(actual, test_pred, digits=4, zero_division=0))\n","    \n","    return actual, test_pred, cm, cm_normalized\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training GNN models"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["%autoreload"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["================================\n","================================\n","Training Model: e_gcn_res\n","Edge label shape: torch.Size([5924874])\n","Edge label unique values: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n","Pred shape: torch.Size([5924874, 15])\n","Model: e_gcn_res -- Epoch: 1 -- Training acc: 0.01  -- Training f1: 0.01 -- Training loss: 2.7410\n","Model: e_gcn_res -- Epoch: 1 -- Validation acc: 1.51 -- Validation f1: 0.30 -- Validation loss: 2.4565\n","Time for validation:  0.9074539020657539 seconds\n","Model: e_gcn_res -- Epoch: 2 -- Training acc: 1.00  -- Training f1: 0.18 -- Training loss: 2.4626\n","Model: e_gcn_res -- Epoch: 2 -- Validation acc: 1.96 -- Validation f1: 0.35 -- Validation loss: 2.2380\n","Time for validation:  1.2886633984744549 seconds\n","Model: e_gcn_res -- Epoch: 3 -- Training acc: 3.01  -- Training f1: 0.67 -- Training loss: 2.2348\n","Model: e_gcn_res -- Epoch: 3 -- Validation acc: 1.60 -- Validation f1: 0.14 -- Validation loss: 2.0587\n","Time for validation:  0.905777096748352 seconds\n","Model: e_gcn_res -- Epoch: 4 -- Training acc: 2.05  -- Training f1: 1.03 -- Training loss: 2.0341\n","Model: e_gcn_res -- Epoch: 4 -- Validation acc: 9.37 -- Validation f1: 14.25 -- Validation loss: 1.9374\n","Time for validation:  1.2467283979058266 seconds\n","Model: e_gcn_res -- Epoch: 5 -- Training acc: 5.47  -- Training f1: 7.33 -- Training loss: 1.9182\n","Model: e_gcn_res -- Epoch: 5 -- Validation acc: 11.98 -- Validation f1: 15.93 -- Validation loss: 1.7812\n","Time for validation:  0.9183743000030518 seconds\n"]}],"source":["for m in my_models:\n","    train_model(m, G, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for m in my_models:\n","    actual, test_pred, cm, cm_normalized = test_model(m.best_model, m.model_name, m.norm, G_test, test_labels, results_final, labels)\n","    # plot_confusion_matrix(cm=cm,\n","    #                       normalize=False,\n","    #                       target_names=labels,\n","    #                       title=f\"Confusion Matrix of {m.model_name}\",\n","    #                       file_path=f\"{confusion_matrices_path}/{m.model_name}.png\")\n","    \n","    # plot_confusion_matrix(cm=cm_normalized,\n","    #                       normalize=False,\n","    #                       normalized=True,\n","    #                       target_names=labels,\n","    #                       title=f\"Normalized Confusion Matrix of {m.model_name}\",\n","    #                       file_path=f\"{confusion_matrices_path}/{m.model_name}_normalized.png\")\n","    \n","    # with open(os.path.join(folder_path, \"actual.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(actual, cls=NumpyEncoder))\n","        \n","    # with open(os.path.join(folder_path, f\"{m.model_name}_pred.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(test_pred, cls=NumpyEncoder))\n","        \n","    # with open(os.path.join(folder_path, \"results.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(results_final, cls=NumpyEncoder))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for m in my_models:\n","    actual, test_pred, cm, cm_normalized = test_model(m.best_model_acc, m.model_name, m.norm, G_test, test_labels, results_final, labels)\n","    # plot_confusion_matrix(cm=cm,\n","    #                       normalize=False,\n","    #                       target_names=labels,\n","    #                       title=f\"Confusion Matrix of {m.model_name}\",\n","    #                       file_path=f\"{confusion_matrices_path}/{m.model_name}.png\")\n","    \n","    # plot_confusion_matrix(cm=cm_normalized,\n","    #                       normalize=False,\n","    #                       normalized=True,\n","    #                       target_names=labels,\n","    #                       title=f\"Normalized Confusion Matrix of {m.model_name}\",\n","    #                       file_path=f\"{confusion_matrices_path}/{m.model_name}_normalized.png\")\n","    \n","    # with open(os.path.join(folder_path, \"actual.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(actual, cls=NumpyEncoder))\n","        \n","    # with open(os.path.join(folder_path, f\"{m.model_name}_pred.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(test_pred, cls=NumpyEncoder))\n","        \n","    # with open(os.path.join(folder_path, \"results.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(results_final, cls=NumpyEncoder))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for m in my_models:\n","    actual, test_pred, cm, cm_normalized = test_model(m.best_model_loss, m.model_name, m.norm, G_test, test_labels, results_final, labels)\n","    # plot_confusion_matrix(cm=cm,\n","    #                       normalize=False,\n","    #                       target_names=labels,\n","    #                       title=f\"Confusion Matrix of {m.model_name}\",\n","    #                       file_path=f\"{confusion_matrices_path}/{m.model_name}.png\")\n","    \n","    # plot_confusion_matrix(cm=cm_normalized,\n","    #                       normalize=False,\n","    #                       normalized=True,\n","    #                       target_names=labels,\n","    #                       title=f\"Normalized Confusion Matrix of {m.model_name}\",\n","    #                       file_path=f\"{confusion_matrices_path}/{m.model_name}_normalized.png\")\n","    \n","    # with open(os.path.join(folder_path, \"actual.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(actual, cls=NumpyEncoder))\n","        \n","    # with open(os.path.join(folder_path, f\"{m.model_name}_pred.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(test_pred, cls=NumpyEncoder))\n","        \n","    # with open(os.path.join(folder_path, \"results.json\"), \"w\") as f:\n","    #     f.writelines(json.dumps(results_final, cls=NumpyEncoder))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# import importlib.util\n","# import sys\n","# def add_lib(module_name, path):\n","#     spec = importlib.util.spec_from_file_location(module_name, path)\n","#     dataset_info = importlib.util.module_from_spec(spec)\n","#     sys.modules[module_name] = dataset_info\n","#     spec.loader.exec_module(dataset_info)\n","\n","# add_lib(\"e_gat\", \"C:/Users/Administrateur/Desktop/GNN-NIDS/src/models/e_gat_my_code.py\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for m in my_models:\n","#     m.best_model = th.load(f\"temp/best_model_{m.model_name}.pth\")\n","#     print(f\"temp/best_model_{m.model_name}.pth\")\n","    \n","#     actual, test_pred, cm, cm_normalized = test_model(m, G_test, test_labels, results_final, labels)\n","#     plot_confusion_matrix(cm=cm,\n","#                           normalize=False,\n","#                           target_names=labels,\n","#                           title=f\"Confusion Matrix of {m.model_name}\",\n","#                           file_path=f\"{confusion_matrices_path}/{m.model_name}.png\")\n","    \n","#     plot_confusion_matrix(cm=cm_normalized,\n","#                           normalize=False,\n","#                           normalized=True,\n","#                           target_names=labels,\n","#                           title=f\"Normalized Confusion Matrix of {m.model_name}\",\n","#                           file_path=f\"{confusion_matrices_path}/{m.model_name}_normalized.png\")\n","    \n","#     with open(os.path.join(folder_path, \"actual.json\"), \"w\") as f:\n","#         f.writelines(json.dumps(actual, cls=NumpyEncoder))\n","        \n","#     with open(os.path.join(folder_path, f\"{m.model_name}_pred.json\"), \"w\") as f:\n","#         f.writelines(json.dumps(test_pred, cls=NumpyEncoder))\n","        \n","#     with open(os.path.join(folder_path, \"results.json\"), \"w\") as f:\n","#         f.writelines(json.dumps(results_final, cls=NumpyEncoder))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4775518,"sourceId":8089266,"sourceType":"datasetVersion"},{"datasetId":4775527,"sourceId":8089281,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
